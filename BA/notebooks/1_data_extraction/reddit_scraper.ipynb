{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4dd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Scraper for Bachelor Thesis Version 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e37ce",
   "metadata": {},
   "source": [
    "# Das ist die erste Version in der neuen Umgebung VS Code, da ich mit Jupyter Notebooks Probleme hatte. Ich hoffe, es wird besser funktionieren, da ich schon eine gewaltige Zeit verschwendet habe, die Probleme vergebens zu lösen. Hier wird wie in der ersten Version in Jupyter Notebooks ein Reddit Scraper programmiert. Ich werde Kommentare in Englisch lassen, da ich so die Codezeilen besser nachvollziehen kann. Ich werde zudem Updates in Deutsch unter dieser Zeile schreiben und nummerieren.\n",
    "\n",
    "#### Update 1: Initiales Setup und Umgebungskonfiguration\n",
    "# Ich habe mich dazu entschieden, meine virtuelle Python-Umgebung \"BA\" zu erstellen und VS Code als meine primäre Entwicklungsumgebung zu wählen. Bereits hier zeigte sich, dass die Probleme mit meiner Python-Installation, die ich zuvor in JupyterLab hatte, behoben werden konnten. Es stellte sich heraus, dass die generelle Python-Installation auf meinem Rechner fehlerhaft war, was wohl auch die Schwierigkeiten mit JupyterLab verursachte. Die Benutzeroberfläche von VS Code gefällt mir besser, und das Programm lädt generell schneller, weshalb ich den Rest meiner Arbeit hier abschließen werde. Um eine bessere Übersicht zu behalten, werde ich einmalige Installationen und Funktionen über das Terminal ausführen. Zudem habe ich black als Code-Formatter und isort für die Sortierung von Imports konfiguriert, um eine konsistente Code-Qualität sicherzustellen. Dies ist mir für die Lesbarkeit und Wartbarkeit des Projekts wichtig.\n",
    "\n",
    "#### Update 2: Projektstruktur und erste Schritte der Datenextraktion\n",
    "# Bislang habe ich das Gerüst für die Datenextraktion erstellt. Ich habe eine umfassende Projektverzeichnisstruktur definiert, die Ordner für Rohdaten, verarbeitete Daten, Notebooks, Quellcode, Berichte und Konfigurationsdateien umfasst. Dies soll die Organisation und Wartbarkeit des Projekts erheblich verbessern. Die Reddit-API-Kredentialien habe ich separat eingetragen, um sensible Informationen nicht direkt im Code zu speichern. Zunächst habe ich eine Handvoll Daten extrahiert, um die grundlegende Struktur und Funktionalität des Codes zu testen.\n",
    "\n",
    "#### Update 3: Sichere Kredentialverwaltung und Strukturimplementierung\n",
    "# Das Grundgerüst ist nun funktionsfähig, und ich habe erfolgreich Zugang zur Reddit API erhalten. Die Reddit API-Kredentialien sind nun in einer externen .env-Datei gespeichert, um den Best Practices der sicheren Datenverwaltung zu folgen. Um zu verhindern, dass diese sensiblen Informationen versehentlich veröffentlicht werden, habe ich die .env-Datei zusätzlich in die .gitignore-Datei aufgenommen. Die .gitignore-Datei weist Git an, welche Dateien und Ordner ignoriert werden sollen, sodass die .env-Datei nicht in das Repository hochgeladen wird. Für die Reddit API-Kredentialien habe ich eine eigene Anwendung in meinem Reddit-Account erstellt und die notwendigen Schlüssel dort entnommen. Die geplanten Hauptverzeichnisse des Stammverzeichnisses wurden ebenfalls angelegt.\n",
    "\n",
    "#### Update 4: Verfeinerung der Datenextraktionslogik\n",
    "# Nach weiterer Überlegung habe ich mich dazu entschieden, nur die Top-Kommentare zu extrahieren und Sub-Kommentare zu ignorieren. Obwohl Sub-Kommentare die Tiefe der Diskussion zeigen, sind sie stark vom Kontext des übergeordneten Kommentars abhängig und vor allem schwieriger zu vergleichen. Die Konzentration auf Top-Kommentare spart zudem Zeit bei der Abbildung komplexer Baumstrukturen und reicht für die Bewertung der Diskussionsqualität aus. Ich habe die post_limit und comment_limit angepasst, um etwa 800 Kommentare pro Turnier zu extrahieren, basierend auf meinen Schätzungen von 20-40 Posts pro Tag und 40-50 Top-Kommentaren pro Post. Redundanter Beispiel-Code für die allgemeine PRAW-API-Nutzung wurde entfernt, um den Code schlanker und fokussierter zu gestalten.\n",
    "\n",
    "#### Update 5: NLTK-Integration und erste Datenbereinigung\n",
    "# Für die Textanalyse habe ich die NLTK-Bibliothek integriert. Die notwendigen NLTK-Datenpakete (stopwords, punkt, wordnet, vader_lexicon) lade ich einmalig über das Terminal herunter, um das Notebook übersichtlich zu halten und unnötige Downloads bei jeder Ausführung zu vermeiden. Die ersten Schritte der Datenbereinigung habe ich bereits implementiert: Duplikate und leere Kommentare werden entfernt. Zudem filtere ich Kommentare von [deleted]-Autoren und solche heraus, deren Inhalt nach der Vorverarbeitung leer wäre, da sie für die Analyse nicht relevant sind.\n",
    "\n",
    "#### Update 6: Erweiterte Textverarbeitung und Sentiment-Analyse\n",
    "# Die Textverarbeitung habe ich nun vertieft. Ich habe eine Pipeline zur Bereinigung, Tokenisierung, Stop-Word-Entfernung und Lemmatisierung implementiert, um den Text optimal für die Analyse vorzubereiten. Dies beinhaltet das Entfernen von URLs, Sonderzeichen, Zahlen und die Umwandlung in Kleinbuchstaben, um den Text für die Analyse zu standardisieren. Anschließend wende ich die VADER-Sentiment-Analyse an, um die Polarität (positiv, neutral, negativ) und die Intensität des Sentiments in jedem Kommentar zu bestimmen. Zusätzlich berechne ich die Textlänge (Zeichen- und Wortanzahl) der Kommentare, da diese Metriken später für die Analyse des Engagements relevant sein könnten.\n",
    "\n",
    "#### Update 7: Zeitliche Kontextualisierung und Verhaltensanalyse\n",
    "# Um die Kommentare im zeitlichen Kontext der Turniere zu analysieren, habe ich Zeitperioden (Before Event, During Event, After Event) basierend auf den Turnierdaten markiert. Den zeitlichen Abstand jedes Kommentars zum Event-Start habe ich in Tagen berechnet, um die Dynamik besser zu verstehen. Erste Visualisierungen habe ich erstellt, um das Posting-Verhalten (Kommentare pro Stunde und Wochentag mittels Histogrammen und Heatmaps) und die Entwicklung der durchschnittlichen Kommentar-Scores im Zeitverlauf zu untersuchen. Diese ersten Einblicke sind entscheidend, um die Daten zu erfassen.\n",
    "\n",
    "#### Update 8: Umfassendes Feature Engineering und Modularisierung\n",
    "# Das Feature Engineering habe ich erheblich erweitert. Ich habe neue Features wie event_name, post_title_length, post_title_word_count, contains_question, contains_team_name, comment_to_post_score_ratio, author_karma (als Platzhalter) und comment_score_per_day hinzugefügt. Ein großer Schritt war die Modularisierung des Codes. Die Kernfunktionen für das Scraping (get_reddit_instance, get_posts_and_comments) habe ich in src/data/reddit_scraper.py ausgelagert. Alle textbezogenen Verarbeitungs- und Feature-Engineering-Funktionen (clean_text, preprocess_text, get_sentiment_score, calculate_text_length, contains_any_keyword) habe ich in src/features/text_features.py verschoben. Die Listen für Teamnamen und andere Keywords habe ich in separate JSON-Konfigurationsdateien (config/teams.json, config/keywords.json) ausgelagert, um die Wartbarkeit zu verbessern. Zudem habe ich eine Fehlerbehandlung in den PRAW-API-Aufrufen implementiert, um die Robustheit des Scrapers zu erhöhen. Die bereinigten und vorverarbeiteten Daten speichere ich nun effizient in einer SQLite-Datenbank, um eine schnelle Wiederverwendung zu ermöglichen.\n",
    "\n",
    "#### Update 9: Datenintegration und erste Analysen\n",
    "# Die bereinigten und mit Features angereicherten Daten speichere ich nun in einer SQLite-Datenbank (reddit_dota2_analysis.db) im data/processed/-Verzeichnis. Dafür habe ich Funktionen zur Datenbankverbindung, Tabellenerstellung (für Posts und Kommentare mit relationalen Beziehungen) und zum Einfügen der Daten implementiert. Nach der Speicherung lade ich die Daten direkt aus der Datenbank, um die weitere Analyse zu ermöglichen. Erste Pearson-Korrelationen zwischen den numerischen Features habe ich berechnet und visualisiert, um erste Einblicke in die Beziehungen zwischen den Variablen zu gewinnen. Anschließend habe ich die erste explorative Datenanalyse (EDA) durchgeführt. Die Daten beider Turniere (TI8 und TI11) habe ich in einem kombinierten DataFrame geladen, um vergleichende Analysen zu ermöglichen. Visualisierungen, um die Verteilung wichtiger Features wie Sentiment-Scores, Kommentar-Scores und Kommentarlängen zu untersuchen, habe ich erstellt. Zudem habe ich die Verteilung der Kommentare über die definierten Zeitperioden (Before Event, During Event, After Event) analysiert und den durchschnittlichen Sentiment-Score sowie den Kommentar-Score pro Zeitperiode für jedes Turnier visualisiert.\n",
    "\n",
    "#### Update 10: ML-Vorbereitung\n",
    "# Für die Machine Learning Vorbereitung habe ich Textdaten der Kommentare mittels TF-IDF-Vektorisierung in eine numerische Sparse Matrix umgewandelt. Parameter wie max_features, min_df, max_df und ngram_range habe ich konfiguriert, um die Dimensionalität zu steuern und relevante Features zu extrahieren. Zudem habe ich alle vorbereiteten Features (TF-IDF, numerische, kategoriale) zu einem einzigen Feature-Set zusammengeführt. Kategoriale Merkmale habe ich mittels One-Hot Encoding in ein numerisches Format umgewandelt. Die comment_score habe ich als Zielvariable für die Modellierung definiert. Die Daten habe ich in Trainings- und Testsets aufgeteilt, um die Modellentwicklung und eine unabhängige Modellbewertung zu ermöglichen. Die Daten sind nun vollständig für das Training von Machine Learning-Modellen vorbereitet. Außerdem habe ich die Code-Struktur neu organisiert, um die Modularität und Wartbarkeit zu verbessern.\n",
    "\n",
    "#### Update 11: Debugging und Optimierung des bestehenden Codes\n",
    "# Das Debugging hat eine erhebliche Zeit in Anspruch genommen. Die Reddit-API lieferte keine Kommentare für historische Daten, da meine Suchanfragen zu spezifisch waren und die Kommentare tief in den Posts versteckt lagen. Die Suchanfragen habe ich Stück für Stück optimiert und subreddit.search() und subreddit.top() kombiniert, um mehr Posts zu finden. Das Abrufen der Kommentare war ineffizient, da replace_more(limit=None) zu viele API-Aufrufe verursachte. Diese habe ich auf limit=3 angepasst, um einen Kompromiss zwischen Datenmenge und Geschwindigkeit zu finden. Um die Effizienz weiter zu steigern, habe ich die Anzahl der Posts, für die Kommentare verarbeitet werden (max_posts_to_process), auf 15 reduziert. Timeouts und API-Fehler waren ebenfalls ein Problem. Hierfür habe ich die tenacity-Bibliothek mit Retry-Mechanismen und Exponential Backoff implementiert, um meine API-Aufrufe robuster zu machen. Diverse KeyError, LookupError und SyntaxError durch fehlende NLTK-Daten, falsche Spaltennamen oder Modularisierungsfehler habe ich ebenfalls behoben.\n",
    "\n",
    "#### Update 12: Fertigstellung der Datenextraktion und -vorbereitung\n",
    "# Die Datenextraktion und -vorbereitung sind nun abgeschlossen. Die Pipeline ist stabil und effizient, die Daten sind gesammelt, bereinigt, mit Features angereichert und in SQLite gespeichert. Die Modularisierung des Codes verbessert die Wartbarkeit erheblich. Ich werde in Zukunft schauen, ob sich meine Worte bewahrheiten, wenn ich einen Spieler genauer unter die Lupe nehmen werde. Vorerst fahre ich aber mit den Turnieren fort und behandle den Spieler als separate Instanz. Jetzt beginne ich mit der Analyse und Modellierung.\n",
    "\n",
    "#### Update 13: Analyse der Turniere TI8 und TI11\n",
    "# Nachdem die Daten erfolgreich geladen und konsolidiert wurden, begann für mich die Phase der Explorativen Datenanalyse (EDA) und Visualisierung. Mein Ziel war es, tiefere Einblicke in die Dynamik des Online-Engagements während der Turniere TI8 und TI11 zu gewinnen. Zunächst habe ich die Verteilung des Compound Sentiment Scores für beide Events untersucht. Hierbei zeigte sich eine bimodale Verteilung mit dominanten Peaks bei neutralem (0.00) und stark positivem (0.75-1.00) Sentiment. Negative Sentiments waren deutlich seltener. Ein Vergleich zwischen TI8 und TI11 offenbarte, dass TI11 tendenziell eine etwas höhere Konzentration an neutralen und stark positiven Kommentaren aufwies, was auf eine möglicherweise sachlichere oder insgesamt positivere Diskussionskultur hindeutet.\n",
    "\n",
    "# Anschließend habe ich die Verteilung des Kommentar-Scores (comment_score) analysiert. Beide Verteilungen waren extrem rechtsschief, was bedeutet, dass die überwiegende Mehrheit der Kommentare einen sehr niedrigen Score (nahe 0) hatte, während nur wenige Kommentare hohe Scores erreichten – ein typisches \"Long Tail\"-Phänomen in sozialen Medien. Interessanterweise zeigte TI11 tendenziell mehr Kommentare mit moderat höheren Scores als TI8, obwohl OGs Sieg bei TI8 als emotionaler wahrgenommen wurde. Dies deutet darauf hin, dass ein hoher Score nicht zwingend mit positivem Sentiment korreliert und andere Faktoren wie Informationsgehalt oder Humor eine Rolle spielen könnten. Die Analyse der Wortanzahl (word_count) der Kommentare bestätigte ebenfalls eine extrem rechtsschiefe Verteilung, wobei die meisten Kommentare sehr kurz waren (Modus bei etwa 10 Wörtern). TI11 wies hier eine höhere Frequenz von kürzeren Kommentaren auf, was auf eine breitere, aber prägnantere Beteiligung hindeuten könnte. Die geringe Korrelation zwischen Wortanzahl und Kommentar-Score unterstreicht, dass die Länge allein kein primärer Treiber für die Popularität eines Kommentars ist.\n",
    "\n",
    "# Ein weiterer Fokus lag auf der zeitlichen Verteilung der Kommentare. Die Analyse zeigte eine extreme Konzentration des Engagements während der Events (\"During Event\"), mit einer überwältigenden Mehrheit der Kommentare in dieser Periode. Die Phasen \"Before Event\" und \"Outside Window\" wiesen keine Kommentare auf, und die Aktivität fiel nach dem Event (\"After Event\") rapide ab. Dieses Muster war für beide Turniere konsistent und unterstreicht die Echtzeit- und reaktive Natur der Diskussionen auf Reddit während großer E-Sports-Events. Die durchschnittlichen Kommentar-Scores nach Zeitperiode zeigten, dass TI8 während des Events höhere, aber variablere Scores hatte, während TI11 homogenere, aber niedrigere Scores aufwies. Nach dem Event schien das Engagement für TI8 etwas nachhaltiger zu sein, während es für TI11 fast auf Null fiel. Diese detaillierten Analysen lieferten wertvolle Erkenntnisse über die unterschiedliche Dynamik des Engagements und der Diskussionskultur zwischen den beiden Turnieren.\n",
    "\n",
    "#### Update 14: Erste Modellierungsversuche und kritische Fehlerbehebung\n",
    "# Nach der umfassenden Explorativen Datenanalyse (Update 13) war der nächste logische Schritt die Anwendung von Machine Learning-Modellen. Mein Ziel war es, das Engagement der Kommentare (comment_score) vorherzusagen und erste Hypothesen über die treibenden Faktoren zu überprüfen.\n",
    "\n",
    "# Ich begann mit der Implementierung eines Multiple Linearen Regressionsmodells als unsere Baseline. Dieses Modell sollte die linearen Beziehungen zwischen unseren aufbereiteten Features und der Zielvariable aufzeigen. Parallel dazu habe ich einen XGBoost Regressor trainiert, ein leistungsstärkeres Ensemble-Modell, das auch komplexe, nicht-lineare Muster in den Daten erkennen kann. Beide Modelle habe ich auf einem separaten Testset evaluiert, um ihre Vorhersagekraft zu beurteilen.\n",
    "\n",
    "# Ein kritischer Moment in dieser Phase war die Beobachtung einer unrealistisch perfekten Modellleistung (R²-Werte von 1.00 und MSE-Werte nahe 0.00). Eine sofortige Analyse der Feature Importance des XGBoost-Modells enthüllte die Ursache: Die Zielvariable (comment_score) war unbeabsichtigt selbst als Feature in den Trainingsdaten enthalten. Dies führte zu einem klassischen Datenleck (Data Leakage), bei dem das Modell die Antwort bereits \"kannte\". Dieser Fehler wurde umgehend behoben, indem die Zielvariable konsequent aus dem Feature-Set entfernt wurde.\n",
    "\n",
    "# Nach dieser entscheidenden Korrektur zeigten die Modelle realistische Leistungswerte, die eine fundierte Analyse ermöglichten. Die Feature Importance des XGBoost-Modells lieferte nun aussagekräftige Einblicke, welche Faktoren den comment_score tatsächlich beeinflussten. Die trainierten Modelle habe ich anschließend persistent gespeichert, um die Reproduzierbarkeit zu gewährleisten und eine spätere Nutzung zu ermöglichen.\n",
    "\n",
    "#### Update 15: Modularisierung der Modellierung und Pipeline-Integration\n",
    "# Um die Robustheit, Wartbarkeit und Skalierbarkeit meiner Machine Learning-Pipeline zu verbessern, habe ich eine umfassende Modularisierung des gesamten Modellierungsprozesses vorgenommen. Die zuvor im Notebook enthaltenen Schritte für die ML-Vorbereitung, das Training, Tuning und die Evaluierung habe ich in dedizierte Python-Skripte ausgelagert.\n",
    "\n",
    "# Der Kern dieser Neuausrichtung ist die Einführung von Scikit-learn Pipelines. Die gesamte Datenvorverarbeitung – von der TF-IDF-Vektorisierung der Texte über das One-Hot-Encoding kategorialer Merkmale bis hin zum Feature Scaling numerischer Variablen – habe ich in einer ColumnTransformer-basierten Pipeline gekapselt. Diese Pipeline stellt sicher, dass alle Transformationen konsistent und ohne Datenlecks angewendet werden, indem sie nur auf den Trainingsdaten gelernt und dann auf alle weiteren Daten (Testset, Cross-Validation) angewendet wird.\n",
    "\n",
    "# Zusätzlich habe ich fortgeschrittene Validierungs- und Optimierungstechniken integriert: Cross-Validation wurde für beide Modelle (Lineare Regression und XGBoost) implementiert, um eine robustere Schätzung der Modellleistung zu ermöglichen. Für das leistungsstarke XGBoost-Modell habe ich eine RandomizedSearchCV implementiert, um effizient den Hyperparameter-Raum zu durchsuchen und die optimale Konfiguration zu finden. Um die \"Black-Box\" des XGBoost-Modells zu öffnen und die Beiträge einzelner Features besser zu verstehen, habe ich SHAP (SHapley Additive exPlanations)-Werte berechnet, die aussagekräftige Plots zur globalen Feature Importance und zu den Auswirkungen einzelner Features generieren.\n",
    "\n",
    "# Der gesamte Workflow wird nun durch ein zentrales Skript (train_model.py) orchestriert, das die modularen Funktionen aus model_utils.py aufruft. Dies schafft eine saubere Trennung der Verantwortlichkeiten und ermöglicht eine effiziente und reproduzierbare Ausführung der gesamten Machine Learning-Pipeline.\n",
    "\n",
    "#### Update 16: Erfolgreiche Forschung und zukünftige Erweiterung des Analysefokus\n",
    "# Die bisherige Forschungsarbeit war äußerst erfolgreich und hat mir ein tiefes Verständnis für die Dynamik des Online-Engagements auf Reddit im Kontext von E-Sports-Turnieren (TI8 und TI11) ermöglicht. Ich habe eine robuste Datenpipeline etabliert, aussagekräftige Features entwickelt und leistungsstarke Machine Learning-Modelle trainiert, die mir erste Einblicke in die Einflussfaktoren des Kommentar-Engagements geben.\n",
    "\n",
    "# Angesichts dieser soliden Grundlage und der modularen Architektur meines Projekts ist es nun an der Zeit, meinen Analysefokus zu erweitern. Um die Komplexität und Nuancen des Online-Engagements noch umfassender zu beleuchten, werde ich zwei neue Variablen in meine Untersuchung integrieren: Erstens, die Spieler-Personality und Medienpräsenz, mit einem Fokus auf Topson. Hierbei werde ich analysieren, wie die persönliche Aktivität und Medienpräsenz eines ikonischen Spielers wie Topson das Engagement auf Reddit beeinflusst, insbesondere im Kontext seines Comebacks und der damit verbundenen Nostalgie und Hype. Zweitens, Organisationen im Umbruch, mit einem Fokus auf OG in 2024/2025. Diese Variable ermöglicht es mir, die Fanreaktionen und die Diskussionsqualität zu untersuchen, wenn ein ehemals dominantes Team wie OG eine Phase des Umbruchs oder des Misserfolgs durchläuft. Dies bietet einen spannenden Kontrast zu den Erfolgsphasen, die ich bereits analysiert habe.\n",
    "\n",
    "# Diese Erweiterung wird es mir ermöglichen, einen analytischen Bogen von \"Erfolg\" über \"Dominanz\" bis hin zu \"Neuorientierung\" zu spannen und die vielfältigen Ausdrucksformen des Fan-Engagements bei Turnieren, Teams und individuellen Spielern zu erforschen. Die Integration dieser neuen Variablen sollte dank unserer modularen Codebasis mit minimalem Aufwand möglich sein, da die bestehenden Datenextraktions-, Vorverarbeitungs- und Modellierungsschritte nahtlos wiederverwendet werden können. Es ist ein absichtlicher Test, um zu sehen, ob meine Codestruktur tatsächlich gut genug ist.\n",
    "\n",
    "#### Update 17: Erweiterung des Datensatzes - Integration von Riyadh Masters 2024\n",
    "# Nachdem die modulare Pipeline für die Datenextraktion und -verarbeitung stabilisiert war, begann die Erweiterung des Datensatzes um die neuen Analysevariablen. Der Fokus lag auf dem Riyadh Masters 2024, einem Turnier, das sowohl die Präsenz von OG (mit schwacher Leistung) als auch von Topson (als Stand-In mit guter Performance) umfasste.\n",
    "\n",
    "# Zunächst erfolgte eine umfassende Aktualisierung der keywords.json-Datei. Diese wurde um alle relevanten Spieler- und Coach-Namen der am Riyadh Masters 2024 teilnehmenden Teams erweitert. Zudem wurden turnierspezifische Begriffe wie \"riyadh masters\", \"gamers8\" und \"esports world cup\" sowie Platzierungen wie \"ninth place\" bis \"twelveth place\" in die tournament_event_keywords aufgenommen. Die Listen wurden alphabetisch sortiert und Duplikate entfernt, um die Qualität der Feature-Erkennung zu optimieren.\n",
    "\n",
    "# Anschließend wurden im reddit_scraper.ipynb-Notebook neue Abschnitte zur Datensammlung für OG und Topson während des Riyadh Masters 2024 hinzugefügt. Hierfür wurden die bereits etablierten Funktionen zur Datenextraktion (get_posts_and_comments) mit spezifischen Suchanfragen (\"OG\", \"Topson\") und dem relevanten Turnierzeitraum (04. bis 28. Juli 2024) verwendet.\n",
    "\n",
    "# Die neu gesammelten Daten für OG und Topson beim Riyadh Masters 2024 wurden nahtlos in die bestehende Datenbereinigungs- und Feature-Engineering-Pipeline integriert. Alle zuvor implementierten Schritte, wie die initiale Bereinigung, Textvorverarbeitung, Sentiment-Analyse, Zeitperioden-Markierung und die Erstellung weiterer Features (z.B. post_title_features, contains_question, contains_team_name, comment_to_post_score_ratio), wurden konsequent auf die neuen DataFrames angewendet. Jedem neuen Datensatz wurde ein eindeutiger event_name (OG_RM24, Topson_RM24) zugewiesen, um eine differenzierte Analyse zu ermöglichen.\n",
    "\n",
    "# Abschließend wurden alle bereinigten und mit Features angereicherten DataFrames (TI8, TI11, OG_RM24, Topson_RM24) zu einem einzigen großen DataFrame zusammengeführt und in der SQLite-Datenbank gespeichert. Diese Konsolidierung stellt sicher, dass die nachfolgenden Schritte der Explorativen Datenanalyse (EDA) und der Machine Learning-Modellierung automatisch alle Events umfassen und die Vorteile der modularen Projektstruktur voll genutzt werden können.\n",
    "\n",
    "#### Update 18: Finale Phase der Modularisierung und Codeoptimierung\n",
    "# Die letzten Wochen waren eine intensive und zugleich sehr aufschlussreiche Phase der Projektentwicklung. Mein Hauptaugenmerk lag darauf, die Modularität und Wartbarkeit des gesamten Codes auf ein Niveau zu heben, das den hohen Ansprüchen meiner Bachelorarbeit gerecht wird. Es war ein iterativer Prozess, der sich durch viele kleine, aber entscheidende Schritte zog, um die Struktur zu perfektionieren.\n",
    "\n",
    "# Ein großer Schritt war die vollständige Auslagerung der Datenextraktion, -vorbereitung und des Feature Engineerings in die prepare_data.py-Datei. Diese Datei war nicht mein erster Versuch, das Notebook zu entlasten, sondern tatsächlich das allerletzte Modul, das ich in diesem Kontext erstellt habe. Es repräsentiert die Kulmination meiner Modularisierungsbemühungen und ist nun eine robuste, eigenständige Pipeline. Ich habe mich dazu entschieden, die gesamte Reddit API-Initialisierung, die zuvor noch im Notebook stattfand, direkt in prepare_data.py zu integrieren. Das bedeutet, die Funktion prüft nun selbst, ob Daten in der SQLite-Datenbank (reddit_dota2_analysis.db) vorhanden sind, und sammelt sie nur bei Bedarf neu. Das spart nicht nur enorme Zeit bei jedem Durchlauf, sondern schont auch die API-Limits – ein Problem, das mich in Update 11 schon viel Zeit gekostet hat.\n",
    "\n",
    "# Die zentrale Konfiguration in config.py hat sich als absolut unverzichtbar erwiesen. Alle relevanten Parameter, von den Turnierdaten über die API-Limits bis hin zu den Pfaden und Modell-Hyperparametern, sind nun dort gebündelt. Es stellte sich heraus, dass dies der Schlüssel zu einer konsistenten und leicht anpassbaren Codebasis ist, was für die Reproduzierbarkeit meiner Ergebnisse entscheidend ist.\n",
    "\n",
    "# Auch die Visualisierungslogik habe ich weiter optimiert. Ich habe eine neue Orchestrierungsfunktion, generate_all_eda_plots, in plots.py implementiert, die nun alle explorativen Datenanalyse-Plots mit einem einzigen Aufruf generiert. Sogar die Pearson-Korrelationsanalyse, die bisher noch direkt im Notebook stattfand, wurde konsequent in eine eigene Funktion (plot_pearson_correlation) in plots.py ausgelagert. Rückblickend muss ich zugeben, dass die Pearson-Korrelation noch hardcodiert war, weil ich sie schlichtweg übersehen und vergessen hatte – ein gutes Beispiel dafür, wie wichtig eine systematische Überprüfung ist. Das macht die Visualisierungen nicht nur konsistenter, sondern auch viel einfacher zu handhaben.\n",
    "\n",
    "# Parallel dazu habe ich alle unterstützenden Module – database_utils.py, preprocess.py, reddit_scraper.py, feature_engineering.py, text_features.py und config_loader.py – einer umfassenden Überprüfung unterzogen. Dabei wurden kleinere Inkonsistenzen behoben, Logging-Konfigurationen vereinheitlicht und redundante Code-Abschnitte entfernt. Besonders die Anpassung von database_utils.py, die nun nur noch die relevanten Kommentardaten zurückgibt, trägt zur Präzision bei.\n",
    "\n",
    "# Das reddit_scraper.ipynb-Notebook selbst ist nun das, was es sein sollte: ein klarer, übersichtlicher Bericht und Orchestrator meines gesamten Workflows. Redundante Imports wurden entfernt, die Nummerierung der Abschnitte wurde überarbeitet, um eine durchgängige und logische Reihenfolge zu gewährleisten, und die Hinweise zur Datenerfassung wurden für verschiedene Betriebssysteme präzisiert.\n",
    "\n",
    "# Mit diesen umfassenden Anpassungen ist die Kern-Programmierarbeit nun abgeschlossen. Ich habe eine robuste, effiziente und vollständig modularisierte Pipeline geschaffen, die mir die Gewissheit gibt, mich in den kommenden Wochen voll und ganz auf die Analyse der Ergebnisse und das Verfassen meiner Bachelorarbeit konzentrieren zu können. Es ist ein gutes Gefühl, diese technische Grundlage geschaffen zu haben.\n",
    "\n",
    "# Es folgt lediglich nur noch das Debugging und die Analyse von Riyadh Masters 2024. Danach sollte ich ich komplett fertig mit dem Programmier-Aspekt werden.\n",
    "\n",
    "#### Update 19: Debugging und Analyse von Riyadh Masters 2024\n",
    "# Die Zeit seit dem letzten Update war eine Phase intensiver Verfeinerung und des Debuggings, die darauf abzielte, die analytische Tiefe und die Robustheit meiner gesamten Codebasis zu maximieren.\n",
    "\n",
    "# Ein zentraler Schritt war die Optimierung meiner Analysemethoden. Ich habe die Visualisierungsfunktionen in plots.py erheblich erweitert, um nicht nur ansprechende Diagramme zu generieren, sondern diese auch direkt mit quantitativen Metriken zu versehen. Das bedeutet, meine Plots zeigen nun nicht mehr nur visuelle Trends, sondern untermauern diese mit konkreten Mittelwerten, Medianen und Standardabweichungen. Dies ist entscheidend, um \"Schätzungen nach Augenmaß\" zu vermeiden und meine Beobachtungen wissenschaftlich präzise zu belegen. Zudem habe ich die Qualität der Plot-Ausgabe für meine Bachelorarbeit optimiert, indem alle Diagramme nun automatisch in hoher Auflösung und in einem konsistenten Format gespeichert werden. Die generierten Bilder finden sich nun im Ordner Code/BA/reports/figures/ und die detaillierten Logs im Ordner Code/BA/logs/.\n",
    "\n",
    "# Eine Implementierung für statistische Tests war für die Analyse nötig. Ich habe ein neues Modul (statistical_tests.py) erstellt, das Funktionen für unabhängige T-Tests, ANOVA und Chi-Quadrat-Tests enthält. Meine bisherigen visuellen Analysen und \"Schätzungen nach Augenmaß\" reichten nicht aus, um eine vollständige und wissenschaftlich fundierte Antwort auf meine Forschungsfragen zu geben. Diese Tests sind nun integriert, um die beobachteten Unterschiede quantitativ zu untermauern und die Ergebnisse besser zu verstehen und zu vergleichen.\n",
    "\n",
    "# Ich habe die text_features.py um einen automatischen NLTK-Daten-Download-Check erweitert. Dies stellt sicher, dass alle notwendigen Sprachressourcen vorhanden sind, bevor die Textverarbeitung beginnt, und macht das Modul widerstandsfähiger gegen fehlende Abhängigkeiten.\n",
    "\n",
    "# Ein erheblicher Teil meiner Zeit floss in das finale Debugging und die Konsistenz des Loggings. Es stellte sich heraus, dass die Art und Weise, wie das Logging in den verschiedenen Modulen konfiguriert war, zu unerwartetem Verhalten führte. Ich habe nun eine umfassende Lösung implementiert, bei der logging.basicConfig() nur noch an den zentralen Einstiegspunkten meiner Anwendung (dem Notebook und den direkt ausführbaren Skripten) aufgerufen wird. Alle anderen Module verwenden nun eine logger-Instanz, was eine durchgängige und zuverlässige Protokollierung des gesamten Workflows gewährleistet. Dies war ein hartnäckiges Problem, das viel Geduld erforderte, aber nun ist der Output meiner Skripte klar und vollständig.\n",
    "\n",
    "# Die Zusammenfassung der Machine Learning Ergebnisse wurde ebenfalls hinzugefügt. Ich muss zugeben, dass diese Funktion anfangs übersehen wurde und ich ihre Notwendigkeit erst jetzt bemerkt habe. Sie speichert die wichtigsten Performance-Metriken und Hyperparameter in einer separaten Textdatei, was für die Dokumentation meiner Arbeit sehr nützlich ist. Gleiches gilt für die Speicherung der Rohdaten: Auch diese wurde erst jetzt implementiert, um die Reproduzierbarkeit meiner Datenbasis zu gewährleisten.\n",
    "\n",
    "# Schließlich habe ich die Projektstruktur noch einmal kritisch beleuchtet und bestätigt, dass die aktuelle Organisation meiner Ordner (BA/config, BA/data/raw, BA/data/processed, BA/models, BA/reports, BA/logs etc.) den Best Practices entspricht und eine klare Trennung der Verantwortlichkeiten gewährleistet. Die finale Runde zum Überschauen aller Module und Funktionen ist damit bereits passiert.\n",
    "\n",
    "# Ich denke, dass die Codebasis für meine Bachelorarbeit genügend ist und ich hiermit hoffentlich fertig bin zu codieren. Das nächste Update wird sich ausschließlich den Auswertungen der Analysen widmen und damit der letzte Teil des Programmierabschnitts sein.\n",
    "\n",
    "# Update 20: Analyse aller Variablen und abschließende Worte zum Programmierteil\n",
    "\n",
    "# \n",
    "\n",
    "# \n",
    "\n",
    "# \n",
    "\n",
    "# \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0141200",
   "metadata": {},
   "source": [
    "# ### **Comprehensive Project Reproduction Guide in VS Code**\n",
    "#\n",
    "# This section provides a detailed, step-by-step guide to fully reproduce this project. It is highly recommended to use Visual Studio Code (VS Code) as your development environment, as it was specifically designed for seamless integration with Python and Jupyter Notebooks.\n",
    "#\n",
    "# #### **1. Prerequisites & Initial Setup**\n",
    "#\n",
    "# Before you begin, ensure the following prerequisites are met:\n",
    "#\n",
    "# *   **Visual Studio Code:** Installed on your system.\n",
    "# *   **Python 3.x:** A recent version of Python is installed.\n",
    "#\n",
    "# **Steps:**\n",
    "#\n",
    "# 1.  **Open the Project Folder:**\n",
    "#     *   Download or clone the entire project repository.\n",
    "#     *   Open VS Code.\n",
    "#     *   Go to `File` > `Open Folder...` and select the main project directory (e.g., `C:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code`).\n",
    "#\n",
    "# 2.  **Install Essential VS Code Extensions:**\n",
    "#     These extensions are crucial for working seamlessly with Python code and Jupyter Notebooks in VS Code.\n",
    "#     *   Open the Extensions view in VS Code (Ctrl+Shift+X or Cmd+Shift+X).\n",
    "#     *   Search for and install the following extensions by Microsoft:\n",
    "#         *   **Python:** Provides IntelliSense, debugging, code formatting, and more.\n",
    "#         *   **Jupyter:** Enables opening, editing, and running `.ipynb` files directly within VS Code.\n",
    "#     *   **Note:** These two extensions are fully sufficient for reproducing the project's results. Other extensions are generally for development convenience but are not strictly necessary for this purpose.\n",
    "#\n",
    "# #### **2. Set Up the Virtual Environment**\n",
    "#\n",
    "# A virtual environment isolates project dependencies from other Python projects on your system, ensuring consistent results.\n",
    "#\n",
    "# 1.  **Open the Integrated Terminal:**\n",
    "#     *   In VS Code, go to `Terminal` > `New Terminal`. This will open a terminal session (typically PowerShell on Windows) directly within your project's root directory.\n",
    "#\n",
    "# 2.  **Adjust PowerShell Execution Policy (Windows Only):**\n",
    "#     *   If you are on Windows and encounter an error like \"cannot be loaded because running scripts is disabled on this system\" when activating the virtual environment, you need to temporarily bypass the execution policy.\n",
    "#     *   In the **VS Code Terminal**, run the following command. Confirm the change by typing `J` (Yes) or `A` (All) and pressing `Enter`. This change is valid only for the current terminal session.\n",
    "#         ```powershell\n",
    "#         Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n",
    "#         ```\n",
    "#\n",
    "# 3.  **Create a Virtual Environment:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python -m venv venv\n",
    "#         ```\n",
    "#     *   **Expected Output:** There should be no error messages. A new folder named `venv` will be created in your project directory.\n",
    "#\n",
    "# 4.  **Activate the Virtual Environment:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         .\\venv\\Scripts\\activate\n",
    "#         ```\n",
    "#     *   **Expected Output:** Your terminal prompt should change to include `(venv)` at the beginning (e.g., `(venv) PS C:\\Your\\Project\\Path>`). This confirms the virtual environment is active.\n",
    "#\n",
    "# 5.  **Select the Python Interpreter in VS Code:**\n",
    "#     *   This is a **critical step** for VS Code to use the Python version from your virtual environment.\n",
    "#     *   In the **bottom-left corner** OR **upper-right corner** of the VS Code window, click on the displayed Python version (e.g., `Python 3.x.x`).\n",
    "#     *   A list of interpreters will appear at the top. Select the interpreter that points to your newly created virtual environment (it will typically have `('venv')` next to it, e.g., `Python 3.x.x ('venv')`).\n",
    "#     *   **Expected Output:** The Python version in the bottom bar should now display `Python 3.x.x (venv)`.\n",
    "#\n",
    "# 6.  **Install Dependencies:**\n",
    "#     *   Ensure the `requirements.txt` file is present in your project's root directory.\n",
    "#     *   In the **VS Code Terminal** (with `(venv)` active), run:\n",
    "#         ```bash\n",
    "#         pip install -r requirements.txt\n",
    "#         ```\n",
    "#     *   **Expected Output:** A series of messages indicating the installation of required libraries. It should conclude with `Successfully installed ...`.\n",
    "#\n",
    "# #### **3. Execute the Project Workflow**\n",
    "#\n",
    "# The project workflow is structured into three main steps: Data Preparation, Exploratory Data Analysis (EDA) & Statistical Tests, and Machine Learning.\n",
    "#\n",
    "# ##### **3.1 Step 1: Data Preparation (`prepare_data.py`)**\n",
    "#\n",
    "# This script is the initial step. It collects raw data from the Reddit API, performs cleaning and preprocessing, extracts features, and saves the processed data into an SQLite database.\n",
    "# This step will be automatically executed when running the subsequent EDA & Statistical Tests notebook. However, you can also run it independently.\n",
    "# If you wish to manually execute this step, follow these instructions:\n",
    "#\n",
    "# 1.  **Open Terminal & Activate Environment:**\n",
    "#     *   Ensure you have a **VS Code Terminal** open and your virtual environment (`(venv)`) is active.\n",
    "#\n",
    "# 2.  **Ensure Unicode Output for Terminal (Optional, but Recommended):**\n",
    "#     *   To prevent `UnicodeEncodeError` when special characters (like emojis in log messages) are printed to the terminal, set the `PYTHONIOENCODING` environment variable to `utf-8`. This needs to be done in each new terminal session where you intend to run Python scripts.\n",
    "#         ```powershell\n",
    "#         $env:PYTHONIOENCODING=\"utf-8\"\n",
    "#         ```\n",
    "#     *   **Clarification:** This instruction is specifically for the terminal output of `.py` scripts. Jupyter Notebooks handle their output differently, so this step is not strictly necessary for notebook cells themselves, but crucial for direct script execution in the terminal.\n",
    "#\n",
    "# 3.  **Execute Data Preparation Script:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python \"BA\\src\\data\\prepare_data.py\"\n",
    "#         ```\n",
    "#     *   **Expected Outputs in Terminal:**\n",
    "#         *   Detailed log messages about the progress of data extraction, cleaning, and feature engineering.\n",
    "#         *   Messages like \"Successfully collected X comments for event_name.\"\n",
    "#         *   Confirmation of database saving: \"Data storage in SQLite complete.\"\n",
    "#         *   Summary of prepared data: \"Data preparation successful. Prepared DataFrame shape: (X, Y)\".\n",
    "#         *   **Important Note:** If you make changes to the data processing logic and wish to re-collect data, you **MUST** manually delete the existing SQLite database file (`reddit_dota2_analysis.db` located in the `BA\\data\\processed` folder) before running this script again.\n",
    "#\n",
    "# ##### **3.2 Step 2: Exploratory Data Analysis (EDA) & Statistical Tests (Jupyter Notebook)**\n",
    "#\n",
    "# This Jupyter Notebook (`reddit_scraper.ipynb`) loads the prepared data, performs exploratory data analysis, generates visualizations, and conducts statistical tests.\n",
    "#\n",
    "# 1.  **Open the Notebook:**\n",
    "#     *   Navigate in the VS Code Explorer to the desired `.ipynb` file (e.g., `BA\\notebooks\\1_data_extraction\\reddit_scraper.ipynb`).\n",
    "#     *   Click on the file to open it in VS Code.\n",
    "#\n",
    "# 2.  **Execute Cells:**\n",
    "#     *   **Recommendation: Use \"Run All\".** While you can execute individual code cells by clicking the \"Play\" icon next to them or using `Shift + Enter`, it is **highly recommended to use the \"Run All\" option** (located at the top-right of the notebook window). This ensures that all cells are executed in the correct order, preventing potential issues with dependencies or state that might arise from executing cells individually or out of order.\n",
    "#     *   **Note on Markdown Cells:** When you run \"Run All\", this markdown cell (the guide itself) and the above will also be executed and displayed. You can easily collapse or hide its output by double-clicking on it. This will return it to its edit form, making the notebook cleaner for viewing. This is particularly useful for large informational cells like this one, which might otherwise appear prominently in the notebook output.\n",
    "#\n",
    "# 3.  **Expected Outputs within the Notebook:**\n",
    "#     *   **Logs:** `logging.info` messages will be displayed directly below the code cells.\n",
    "#     *   **Data Loading:** Checks will be performed to see if data is already present in the SQLite database. If so, it will load the existing data for further use. If not, data scraping processes will be initiated via `prepare_data.py` (though this step is primarily handled by the `prepare_data.py` script itself).\n",
    "#     *   **Plots:** Generated charts (e.g., correlation matrices, distribution plots) will be embedded directly within the output cells of the notebook via `plots.py`.\n",
    "#     *   **Statistical Test Results:** The results of statistical tests will be printed in the output cells via `statistical_tests.py`.\n",
    "#     *   **Confirmation Messages:** Messages like \"Combined DataFrame successfully created with shape: (X, Y)\" or \"Comments DataFrame loaded with shape: (X, Y)\" confirm progress.\n",
    "#\n",
    "# ##### **3.3 Step 3: Machine Learning Pipeline (`train_model.py`)**\n",
    "#\n",
    "# This script executes the machine learning workflow, including model training, hyperparameter tuning, evaluation, and saving of trained artifacts.\n",
    "#\n",
    "# 1.  **Open Terminal & Activate Environment:**\n",
    "#     *   Ensure you have a **VS Code Terminal** open and your virtual environment (`(venv)`) is active.\n",
    "#\n",
    "# 2.  **Execute Machine Learning Pipeline Script:**\n",
    "#     *   In the **VS Code Terminal**, run:\n",
    "#         ```bash\n",
    "#         python \"BA\\src\\models\\train_model.py\"\n",
    "#         ```\n",
    "#     *   **Expected Outputs in Terminal:**\n",
    "#         *   Detailed log messages about the progress of model training and evaluation.\n",
    "#         *   Performance metrics for Linear Regression and XGBoost (e.g., R² values, MSE).\n",
    "#         *   Information about hyperparameter tuning (best parameters).\n",
    "#         *   Messages regarding the generation and saving of SHAP plots (e.g., \"SHAP Summary Plot saved to: ...\").\n",
    "#         *   Confirmation of model artifact saving: \"Models and preprocessing objects saved to: ...\"\n",
    "#         *   Final completion message: \"Model Pipeline Completed Successfully.\"\n",
    "#\n",
    "# #### **4. Viewing Results**\n",
    "#\n",
    "# After successfully executing the project workflow, you can examine the generated outputs:\n",
    "#\n",
    "# 1.  **Explore Generated Files:**\n",
    "#     *   **Data:** The SQLite database (`reddit_dota2_analysis.db`) will be located in `BA\\data\\processed`. You can open it using SQLite viewers or directly within VS Code using the SQLite extension.\n",
    "#     *   **Plots & Visualizations:** All generated plots and visualizations will be saved in the `BA\\reports\\figures` directory. You can open these images using any image viewer.\n",
    "#     *   **Model Artifacts:** The trained models and preprocessing objects will be saved in the `BA\\models` directory. These can be loaded and used for inference or further analysis.\n",
    "#\n",
    "# 2.  **Logging:**\n",
    "#     *   All logs for `prepare_data.py` and the Notebook are saved in `BA\\logs\\model_pipeline.log`. This file contains detailed logs from both the data preparation and model training processes.\n",
    "#     *   A precise gathering of data outputs and visualizations for ML models can be also found in `BA\\reports\\figures`. Summaries of the model's performance and SHAP plots are saved here too.\n",
    "#\n",
    "# **End of Guide**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 16:15:56,127 - INFO - NLTK data 'stopwords' already downloaded.\n",
      "2025-07-31 16:15:56,128 - INFO - NLTK data 'punkt' already downloaded.\n",
      "2025-07-31 16:15:56,131 - INFO - NLTK data 'wordnet' not found. Attempting to download now...\n",
      "2025-07-31 16:15:56,342 - INFO - NLTK data 'wordnet' downloaded successfully.\n",
      "2025-07-31 16:15:56,344 - INFO - NLTK data 'vader_lexicon' not found. Attempting to download now...\n",
      "2025-07-31 16:15:56,345 - INFO - NLTK data 'vader_lexicon' downloaded successfully.\n",
      "2025-07-31 16:15:56,355 - INFO - Starting data preparation and feature engineering pipeline...\n",
      "2025-07-31 16:15:56,355 - INFO - --- Starting Data Preparation Pipeline ---\n",
      "2025-07-31 16:15:56,360 - INFO - Database 'reddit_dota2_analysis.db' at 'c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\data\\processed\\reddit_dota2_analysis.db' and table 'comments_data' exist and contain 2642 rows.\n",
      "2025-07-31 16:15:56,360 - INFO - \n",
      "Processed data found in 'reddit_dota2_analysis.db'. Loading data directly from database.\n",
      "2025-07-31 16:15:56,361 - INFO - Attempting to load data from SQLite database: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\data\\processed\\reddit_dota2_analysis.db\n",
      "2025-07-31 16:15:56,395 - INFO - Successfully loaded 'comments_data' table with 2642 rows from 'c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\data\\processed\\reddit_dota2_analysis.db'.\n",
      "2025-07-31 16:15:56,396 - INFO - Loaded 2642 rows from database.\n",
      "2025-07-31 16:15:56,396 - INFO - --- Data Preparation Pipeline Completed ---\n",
      "2025-07-31 16:15:56,397 - INFO - Data preparation and feature engineering pipeline complete.\n",
      "2025-07-31 16:15:56,397 - INFO - Combined DataFrame successfully created with shape: (2642, 41)\n",
      "2025-07-31 16:15:56,397 - INFO - Combined Cleaned DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2642 entries, 0 to 2641\n",
      "Data columns (total 41 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   post_id                      2642 non-null   object \n",
      " 1   post_title                   2642 non-null   object \n",
      " 2   post_url                     2642 non-null   object \n",
      " 3   post_author                  2642 non-null   object \n",
      " 4   post_created_utc             2642 non-null   object \n",
      " 5   post_score                   2642 non-null   int64  \n",
      " 6   post_num_comments            2642 non-null   int64  \n",
      " 7   upvote_ratio                 2642 non-null   float64\n",
      " 8   is_self                      2642 non-null   int64  \n",
      " 9   selftext                     2642 non-null   object \n",
      " 10  link_flair_text              2604 non-null   object \n",
      " 11  permalink                    2642 non-null   object \n",
      " 12  comment_id                   2642 non-null   object \n",
      " 13  comment_body                 2642 non-null   object \n",
      " 14  comment_author               2642 non-null   object \n",
      " 15  comment_created_utc          2642 non-null   object \n",
      " 16  comment_score                2642 non-null   int64  \n",
      " 17  comment_permalink            2642 non-null   object \n",
      " 18  processed_comment_body       2642 non-null   object \n",
      " 19  neg_sentiment                2642 non-null   float64\n",
      " 20  neu_sentiment                2642 non-null   float64\n",
      " 21  pos_sentiment                2642 non-null   float64\n",
      " 22  compound_sentiment           2642 non-null   float64\n",
      " 23  char_count                   2642 non-null   int64  \n",
      " 24  word_count                   2642 non-null   int64  \n",
      " 25  time_period                  2642 non-null   object \n",
      " 26  days_from_event_start        2642 non-null   int64  \n",
      " 27  post_title_length            2642 non-null   int64  \n",
      " 28  post_title_word_count        2642 non-null   int64  \n",
      " 29  contains_question            2642 non-null   int64  \n",
      " 30  author_karma                 2642 non-null   int64  \n",
      " 31  contains_team_name           2642 non-null   int64  \n",
      " 32  contains_player_keyword      2642 non-null   int64  \n",
      " 33  contains_hero_keyword        2642 non-null   int64  \n",
      " 34  contains_event_keyword       2642 non-null   int64  \n",
      " 35  post_type                    2642 non-null   object \n",
      " 36  comment_to_post_score_ratio  2642 non-null   float64\n",
      " 37  comment_score_per_day        2642 non-null   float64\n",
      " 38  event_name                   2642 non-null   object \n",
      " 39  comment_hour                 2642 non-null   int64  \n",
      " 40  comment_day_of_week          2642 non-null   object \n",
      "dtypes: float64(7), int64(16), object(18)\n",
      "memory usage: 846.4+ KB\n",
      "\n",
      "Descriptive statistics of the combined DataFrame:\n",
      "        post_id                                         post_title  \\\n",
      "count      2642                                               2642   \n",
      "unique       45                                                 45   \n",
      "top     1e8rg6s  Your face while watching your supports picking...   \n",
      "freq        100                                                100   \n",
      "mean        NaN                                                NaN   \n",
      "std         NaN                                                NaN   \n",
      "min         NaN                                                NaN   \n",
      "25%         NaN                                                NaN   \n",
      "50%         NaN                                                NaN   \n",
      "75%         NaN                                                NaN   \n",
      "max         NaN                                                NaN   \n",
      "\n",
      "                               post_url          post_author  \\\n",
      "count                              2642                 2642   \n",
      "unique                               45                   36   \n",
      "top     https://v.redd.it/bhaagwzlowdd1  D2TournamentThreads   \n",
      "freq                                100                  346   \n",
      "mean                                NaN                  NaN   \n",
      "std                                 NaN                  NaN   \n",
      "min                                 NaN                  NaN   \n",
      "25%                                 NaN                  NaN   \n",
      "50%                                 NaN                  NaN   \n",
      "75%                                 NaN                  NaN   \n",
      "max                                 NaN                  NaN   \n",
      "\n",
      "           post_created_utc    post_score  post_num_comments  upvote_ratio  \\\n",
      "count                  2642   2642.000000        2642.000000   2642.000000   \n",
      "unique                   45           NaN                NaN           NaN   \n",
      "top     2024-07-21 17:22:56           NaN                NaN           NaN   \n",
      "freq                    100           NaN                NaN           NaN   \n",
      "mean                    NaN   1950.809235         981.584406      0.952555   \n",
      "std                     NaN   2648.047116        2447.948603      0.039402   \n",
      "min                     NaN    149.000000           8.000000      0.730000   \n",
      "25%                     NaN    601.000000         116.000000      0.940000   \n",
      "50%                     NaN   1773.000000         193.000000      0.960000   \n",
      "75%                     NaN   2617.000000         358.000000      0.980000   \n",
      "max                     NaN  20844.000000       13295.000000      0.990000   \n",
      "\n",
      "            is_self selftext  ... contains_team_name contains_player_keyword  \\\n",
      "count   2642.000000     2642  ...        2642.000000             2642.000000   \n",
      "unique          NaN       24  ...                NaN                     NaN   \n",
      "top             NaN           ...                NaN                     NaN   \n",
      "freq            NaN     1616  ...                NaN                     NaN   \n",
      "mean       0.388342      NaN  ...           0.145344                0.211204   \n",
      "std        0.487465      NaN  ...           0.352514                0.408240   \n",
      "min        0.000000      NaN  ...           0.000000                0.000000   \n",
      "25%        0.000000      NaN  ...           0.000000                0.000000   \n",
      "50%        0.000000      NaN  ...           0.000000                0.000000   \n",
      "75%        1.000000      NaN  ...           0.000000                0.000000   \n",
      "max        1.000000      NaN  ...           1.000000                1.000000   \n",
      "\n",
      "       contains_hero_keyword contains_event_keyword        post_type  \\\n",
      "count            2642.000000            2642.000000             2642   \n",
      "unique                   NaN                    NaN                4   \n",
      "top                      NaN                    NaN  Player Transfer   \n",
      "freq                     NaN                    NaN             1215   \n",
      "mean                0.213096               0.320590              NaN   \n",
      "std                 0.409573               0.466792              NaN   \n",
      "min                 0.000000               0.000000              NaN   \n",
      "25%                 0.000000               0.000000              NaN   \n",
      "50%                 0.000000               0.000000              NaN   \n",
      "75%                 0.000000               1.000000              NaN   \n",
      "max                 1.000000               1.000000              NaN   \n",
      "\n",
      "       comment_to_post_score_ratio  comment_score_per_day   event_name  \\\n",
      "count                  2642.000000           2.642000e+03         2642   \n",
      "unique                         NaN                    NaN            4   \n",
      "top                            NaN                    NaN  Topson_RM24   \n",
      "freq                           NaN                    NaN          709   \n",
      "mean                      0.051902           1.173402e+04          NaN   \n",
      "std                       0.107235           1.010965e+05          NaN   \n",
      "min                      -0.064378          -2.063037e+02          NaN   \n",
      "25%                       0.006985           4.998312e+01          NaN   \n",
      "50%                       0.017525           1.874881e+02          NaN   \n",
      "75%                       0.055424           7.364944e+02          NaN   \n",
      "max                       1.926966           2.442159e+06          NaN   \n",
      "\n",
      "       comment_hour  comment_day_of_week  \n",
      "count   2642.000000                 2642  \n",
      "unique          NaN                    7  \n",
      "top             NaN               Sunday  \n",
      "freq            NaN                  528  \n",
      "mean      11.008327                  NaN  \n",
      "std        6.841800                  NaN  \n",
      "min        0.000000                  NaN  \n",
      "25%        5.000000                  NaN  \n",
      "50%       11.000000                  NaN  \n",
      "75%       17.000000                  NaN  \n",
      "max       23.000000                  NaN  \n",
      "\n",
      "[11 rows x 41 columns]\n",
      "2025-07-31 16:15:56,442 - INFO - \n",
      "--- Loading data from SQLite database ---\n",
      "2025-07-31 16:15:56,442 - INFO - Comments DataFrame loaded with shape: (2642, 41)\n",
      "2025-07-31 16:15:56,443 - INFO - \n",
      "Comments DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2642 entries, 0 to 2641\n",
      "Data columns (total 41 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   post_id                      2642 non-null   object \n",
      " 1   post_title                   2642 non-null   object \n",
      " 2   post_url                     2642 non-null   object \n",
      " 3   post_author                  2642 non-null   object \n",
      " 4   post_created_utc             2642 non-null   object \n",
      " 5   post_score                   2642 non-null   int64  \n",
      " 6   post_num_comments            2642 non-null   int64  \n",
      " 7   upvote_ratio                 2642 non-null   float64\n",
      " 8   is_self                      2642 non-null   int64  \n",
      " 9   selftext                     2642 non-null   object \n",
      " 10  link_flair_text              2604 non-null   object \n",
      " 11  permalink                    2642 non-null   object \n",
      " 12  comment_id                   2642 non-null   object \n",
      " 13  comment_body                 2642 non-null   object \n",
      " 14  comment_author               2642 non-null   object \n",
      " 15  comment_created_utc          2642 non-null   object \n",
      " 16  comment_score                2642 non-null   int64  \n",
      " 17  comment_permalink            2642 non-null   object \n",
      " 18  processed_comment_body       2642 non-null   object \n",
      " 19  neg_sentiment                2642 non-null   float64\n",
      " 20  neu_sentiment                2642 non-null   float64\n",
      " 21  pos_sentiment                2642 non-null   float64\n",
      " 22  compound_sentiment           2642 non-null   float64\n",
      " 23  char_count                   2642 non-null   int64  \n",
      " 24  word_count                   2642 non-null   int64  \n",
      " 25  time_period                  2642 non-null   object \n",
      " 26  days_from_event_start        2642 non-null   int64  \n",
      " 27  post_title_length            2642 non-null   int64  \n",
      " 28  post_title_word_count        2642 non-null   int64  \n",
      " 29  contains_question            2642 non-null   int64  \n",
      " 30  author_karma                 2642 non-null   int64  \n",
      " 31  contains_team_name           2642 non-null   int64  \n",
      " 32  contains_player_keyword      2642 non-null   int64  \n",
      " 33  contains_hero_keyword        2642 non-null   int64  \n",
      " 34  contains_event_keyword       2642 non-null   int64  \n",
      " 35  post_type                    2642 non-null   object \n",
      " 36  comment_to_post_score_ratio  2642 non-null   float64\n",
      " 37  comment_score_per_day        2642 non-null   float64\n",
      " 38  event_name                   2642 non-null   object \n",
      " 39  comment_hour                 2642 non-null   int64  \n",
      " 40  comment_day_of_week          2642 non-null   object \n",
      "dtypes: float64(7), int64(16), object(18)\n",
      "memory usage: 846.4+ KB\n",
      "\n",
      "Descriptive statistics of the loaded DataFrame:\n",
      "        post_id                                         post_title  \\\n",
      "count      2642                                               2642   \n",
      "unique       45                                                 45   \n",
      "top     1e8rg6s  Your face while watching your supports picking...   \n",
      "freq        100                                                100   \n",
      "mean        NaN                                                NaN   \n",
      "std         NaN                                                NaN   \n",
      "min         NaN                                                NaN   \n",
      "25%         NaN                                                NaN   \n",
      "50%         NaN                                                NaN   \n",
      "75%         NaN                                                NaN   \n",
      "max         NaN                                                NaN   \n",
      "\n",
      "                               post_url          post_author  \\\n",
      "count                              2642                 2642   \n",
      "unique                               45                   36   \n",
      "top     https://v.redd.it/bhaagwzlowdd1  D2TournamentThreads   \n",
      "freq                                100                  346   \n",
      "mean                                NaN                  NaN   \n",
      "std                                 NaN                  NaN   \n",
      "min                                 NaN                  NaN   \n",
      "25%                                 NaN                  NaN   \n",
      "50%                                 NaN                  NaN   \n",
      "75%                                 NaN                  NaN   \n",
      "max                                 NaN                  NaN   \n",
      "\n",
      "           post_created_utc    post_score  post_num_comments  upvote_ratio  \\\n",
      "count                  2642   2642.000000        2642.000000   2642.000000   \n",
      "unique                   45           NaN                NaN           NaN   \n",
      "top     2024-07-21 17:22:56           NaN                NaN           NaN   \n",
      "freq                    100           NaN                NaN           NaN   \n",
      "mean                    NaN   1950.809235         981.584406      0.952555   \n",
      "std                     NaN   2648.047116        2447.948603      0.039402   \n",
      "min                     NaN    149.000000           8.000000      0.730000   \n",
      "25%                     NaN    601.000000         116.000000      0.940000   \n",
      "50%                     NaN   1773.000000         193.000000      0.960000   \n",
      "75%                     NaN   2617.000000         358.000000      0.980000   \n",
      "max                     NaN  20844.000000       13295.000000      0.990000   \n",
      "\n",
      "            is_self selftext  ... contains_team_name contains_player_keyword  \\\n",
      "count   2642.000000     2642  ...        2642.000000             2642.000000   \n",
      "unique          NaN       24  ...                NaN                     NaN   \n",
      "top             NaN           ...                NaN                     NaN   \n",
      "freq            NaN     1616  ...                NaN                     NaN   \n",
      "mean       0.388342      NaN  ...           0.145344                0.211204   \n",
      "std        0.487465      NaN  ...           0.352514                0.408240   \n",
      "min        0.000000      NaN  ...           0.000000                0.000000   \n",
      "25%        0.000000      NaN  ...           0.000000                0.000000   \n",
      "50%        0.000000      NaN  ...           0.000000                0.000000   \n",
      "75%        1.000000      NaN  ...           0.000000                0.000000   \n",
      "max        1.000000      NaN  ...           1.000000                1.000000   \n",
      "\n",
      "       contains_hero_keyword contains_event_keyword        post_type  \\\n",
      "count            2642.000000            2642.000000             2642   \n",
      "unique                   NaN                    NaN                4   \n",
      "top                      NaN                    NaN  Player Transfer   \n",
      "freq                     NaN                    NaN             1215   \n",
      "mean                0.213096               0.320590              NaN   \n",
      "std                 0.409573               0.466792              NaN   \n",
      "min                 0.000000               0.000000              NaN   \n",
      "25%                 0.000000               0.000000              NaN   \n",
      "50%                 0.000000               0.000000              NaN   \n",
      "75%                 0.000000               1.000000              NaN   \n",
      "max                 1.000000               1.000000              NaN   \n",
      "\n",
      "       comment_to_post_score_ratio  comment_score_per_day   event_name  \\\n",
      "count                  2642.000000           2.642000e+03         2642   \n",
      "unique                         NaN                    NaN            4   \n",
      "top                            NaN                    NaN  Topson_RM24   \n",
      "freq                           NaN                    NaN          709   \n",
      "mean                      0.051902           1.173402e+04          NaN   \n",
      "std                       0.107235           1.010965e+05          NaN   \n",
      "min                      -0.064378          -2.063037e+02          NaN   \n",
      "25%                       0.006985           4.998312e+01          NaN   \n",
      "50%                       0.017525           1.874881e+02          NaN   \n",
      "75%                       0.055424           7.364944e+02          NaN   \n",
      "max                       1.926966           2.442159e+06          NaN   \n",
      "\n",
      "       comment_hour  comment_day_of_week  \n",
      "count   2642.000000                 2642  \n",
      "unique          NaN                    7  \n",
      "top             NaN               Sunday  \n",
      "freq            NaN                  528  \n",
      "mean      11.008327                  NaN  \n",
      "std        6.841800                  NaN  \n",
      "min        0.000000                  NaN  \n",
      "25%        5.000000                  NaN  \n",
      "50%       11.000000                  NaN  \n",
      "75%       17.000000                  NaN  \n",
      "max       23.000000                  NaN  \n",
      "\n",
      "[11 rows x 41 columns]\n",
      "2025-07-31 16:15:56,482 - INFO - \n",
      "--- Performing Statistical Tests ---\n",
      "\n",
      "--- Independent Samples t-test for Compound Sentiment ---\n",
      "2025-07-31 16:15:56,483 - INFO - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI8' and 'TI11' ---\n",
      "2025-07-31 16:15:56,486 - INFO -   t-statistic: 1.562\n",
      "2025-07-31 16:15:56,487 - INFO -   p-value: 0.118\n",
      "2025-07-31 16:15:56,487 - INFO -   Interpretation: There is no statistically significant difference (p=0.118) in 'compound_sentiment' between 'TI8' (Mean: 0.24) and 'TI11' (Mean: 0.19).\n",
      "2025-07-31 16:15:56,487 - INFO - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI8' and 'OG_RM24' ---\n",
      "2025-07-31 16:15:56,490 - INFO -   t-statistic: 3.285\n",
      "2025-07-31 16:15:56,490 - INFO -   p-value: 0.001\n",
      "2025-07-31 16:15:56,491 - INFO -   Interpretation: There is a statistically significant difference (p=0.001) in 'compound_sentiment' between 'TI8' (Mean: 0.24) and 'OG_RM24' (Mean: 0.15).\n",
      "2025-07-31 16:15:56,491 - INFO - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'TI11' and 'OG_RM24' ---\n",
      "2025-07-31 16:15:56,494 - INFO -   t-statistic: 1.610\n",
      "2025-07-31 16:15:56,494 - INFO -   p-value: 0.108\n",
      "2025-07-31 16:15:56,494 - INFO -   Interpretation: There is no statistically significant difference (p=0.108) in 'compound_sentiment' between 'TI11' (Mean: 0.19) and 'OG_RM24' (Mean: 0.15).\n",
      "2025-07-31 16:15:56,495 - INFO - \n",
      "--- Performing Independent Samples t-test for 'compound_sentiment' between 'OG_RM24' and 'Topson_RM24' ---\n",
      "2025-07-31 16:15:56,497 - INFO -   t-statistic: -0.052\n",
      "2025-07-31 16:15:56,498 - INFO -   p-value: 0.958\n",
      "2025-07-31 16:15:56,498 - INFO -   Interpretation: There is no statistically significant difference (p=0.958) in 'compound_sentiment' between 'OG_RM24' (Mean: 0.15) and 'Topson_RM24' (Mean: 0.15).\n",
      "\n",
      "--- One-Way ANOVA for Comment Score across Time Periods ---\n",
      "2025-07-31 16:15:56,499 - INFO - \n",
      "--- Performing One-Way ANOVA test for 'comment_score' across groups in 'time_period' ---\n",
      "2025-07-31 16:15:56,502 - INFO -   F-statistic: 0.650\n",
      "2025-07-31 16:15:56,503 - INFO -   p-value: 0.522\n",
      "2025-07-31 16:15:56,503 - INFO -   Interpretation: There is no statistically significant difference (p=0.522) in 'comment_score' across groups in 'time_period'.\n",
      "\n",
      "--- One-Way ANOVA for Compound Sentiment across Time Periods ---\n",
      "2025-07-31 16:15:56,503 - INFO - \n",
      "--- Performing One-Way ANOVA test for 'compound_sentiment' across groups in 'time_period' ---\n",
      "2025-07-31 16:15:56,507 - INFO -   F-statistic: 1.804\n",
      "2025-07-31 16:15:56,507 - INFO -   p-value: 0.165\n",
      "2025-07-31 16:15:56,508 - INFO -   Interpretation: There is no statistically significant difference (p=0.165) in 'compound_sentiment' across groups in 'time_period'.\n",
      "\n",
      "--- One-Way ANOVA for Comment Score across Post Types ---\n",
      "2025-07-31 16:15:56,508 - INFO - \n",
      "--- Performing One-Way ANOVA test for 'comment_score' across groups in 'post_type' ---\n",
      "2025-07-31 16:15:56,511 - INFO -   F-statistic: 6.074\n",
      "2025-07-31 16:15:56,512 - INFO -   p-value: 0.000\n",
      "2025-07-31 16:15:56,512 - INFO -   Interpretation: There is a statistically significant difference (p=0.000) in 'comment_score' across at least two groups in 'post_type'.\n",
      "\n",
      "--- One-Way ANOVA for Compound Sentiment across Post Types ---\n",
      "2025-07-31 16:15:56,513 - INFO - \n",
      "--- Performing One-Way ANOVA test for 'compound_sentiment' across groups in 'post_type' ---\n",
      "2025-07-31 16:15:56,517 - INFO -   F-statistic: 3.807\n",
      "2025-07-31 16:15:56,517 - INFO -   p-value: 0.010\n",
      "2025-07-31 16:15:56,517 - INFO -   Interpretation: There is a statistically significant difference (p=0.010) in 'compound_sentiment' across at least two groups in 'post_type'.\n",
      "\n",
      "--- Chi-squared test for Time Period and Contains Question ---\n",
      "2025-07-31 16:15:56,518 - INFO - \n",
      "--- Performing Chi-squared test of independence between 'time_period' and 'contains_question' ---\n",
      "2025-07-31 16:15:56,524 - INFO -   Chi-squared statistic: 1.934\n",
      "2025-07-31 16:15:56,525 - INFO -   p-value: 0.380\n",
      "2025-07-31 16:15:56,525 - INFO -   Degrees of freedom: 2\n",
      "2025-07-31 16:15:56,525 - INFO -   Interpretation: There is no statistically significant association (p=0.380) between 'time_period' and 'contains_question'. This suggests that the two variables are independent.\n",
      "\n",
      "--- Chi-squared test for Event Name and Contains Team Name ---\n",
      "2025-07-31 16:15:56,526 - INFO - \n",
      "--- Performing Chi-squared test of independence between 'event_name' and 'contains_team_name' ---\n",
      "2025-07-31 16:15:56,530 - INFO -   Chi-squared statistic: 323.792\n",
      "2025-07-31 16:15:56,530 - INFO -   p-value: 0.000\n",
      "2025-07-31 16:15:56,531 - INFO -   Degrees of freedom: 3\n",
      "2025-07-31 16:15:56,531 - INFO -   Interpretation: There is a statistically significant association (p=0.000) between 'event_name' and 'contains_team_name'. This suggests that the two variables are not independent.\n",
      "2025-07-31 16:15:56,531 - INFO - \n",
      "--- Statistical Tests Complete ---\n",
      "2025-07-31 16:15:56,532 - INFO - \n",
      "--- Generating Exploratory Data Analysis Plots ---\n",
      "2025-07-31 16:15:56,954 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_sentiment_compound_sentiment.png\n",
      "2025-07-31 16:15:57,411 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_comment_score_comment_score.png\n",
      "2025-07-31 16:15:57,844 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_word_count_word_count.png\n",
      "2025-07-31 16:15:57,846 - INFO - \n",
      "--- Generating Distribution Comparison Plots (Box/Violin) ---\n",
      "2025-07-31 16:15:58,162 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_comp_sentiment_compound_sentiment_violin.png\n",
      "2025-07-31 16:15:58,439 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_comp_comment_score_comment_score_violin.png\n",
      "2025-07-31 16:15:58,737 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\dist_comp_word_count_word_count_violin.png\n",
      "2025-07-31 16:15:58,798 - INFO - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\comment_distribution_time_periods_table.txt\n",
      "2025-07-31 16:15:59,038 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\comment_distribution_time_periods.png\n",
      "2025-07-31 16:15:59,044 - INFO - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\avg_sentiment_by_time_period_table.txt\n",
      "2025-07-31 16:15:59,403 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\avg_sentiment_by_time_period.png\n",
      "2025-07-31 16:15:59,410 - INFO - Table data saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\avg_comment_score_by_time_period_table.txt\n",
      "2025-07-31 16:15:59,767 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\avg_comment_score_by_time_period.png\n",
      "2025-07-31 16:16:00,297 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\avg_comment_score_by_post_type.png\n",
      "2025-07-31 16:16:00,700 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\avg_sentiment_by_post_type.png\n",
      "2025-07-31 16:16:01,101 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\avg_word_count_by_post_type.png\n",
      "2025-07-31 16:16:01,103 - INFO - \n",
      "--- Calculating Pearson Correlations for Comments Data ---\n",
      "2025-07-31 16:16:01,107 - INFO - Correlation Matrix:\n",
      "                         contains_question  char_count  neu_sentiment  \\\n",
      "contains_question                 1.000000    0.055209       0.036047   \n",
      "char_count                        0.055209    1.000000      -0.056453   \n",
      "neu_sentiment                     0.036047   -0.056453       1.000000   \n",
      "neg_sentiment                    -0.015401    0.032313      -0.478073   \n",
      "contains_player_keyword           0.010936    0.237977      -0.042982   \n",
      "comment_score_per_day             0.002035   -0.007181      -0.025686   \n",
      "post_title_word_count             0.027547    0.047088      -0.044322   \n",
      "contains_event_keyword            0.027547    0.340823      -0.100298   \n",
      "compound_sentiment                0.015967    0.248477      -0.192986   \n",
      "contains_team_name                0.022379    0.222747      -0.067693   \n",
      "word_count                        0.038472    0.961402      -0.070966   \n",
      "pos_sentiment                    -0.024831    0.044858      -0.727184   \n",
      "post_score                       -0.012816    0.031795      -0.023591   \n",
      "comment_hour                      0.014004   -0.019148       0.015016   \n",
      "post_num_comments                 0.003549    0.017785      -0.044544   \n",
      "comment_score                    -0.013400   -0.005910       0.012120   \n",
      "is_self                           0.025078    0.094401      -0.073533   \n",
      "upvote_ratio                     -0.002730   -0.030914       0.026765   \n",
      "days_from_event_start             0.013536    0.118237      -0.106164   \n",
      "post_title_length                 0.027370    0.046571      -0.046688   \n",
      "author_karma                           NaN         NaN            NaN   \n",
      "contains_hero_keyword            -0.002172    0.253023       0.035900   \n",
      "\n",
      "                         neg_sentiment  contains_player_keyword  \\\n",
      "contains_question            -0.015401                 0.010936   \n",
      "char_count                    0.032313                 0.237977   \n",
      "neu_sentiment                -0.478073                -0.042982   \n",
      "neg_sentiment                 1.000000                 0.005245   \n",
      "contains_player_keyword       0.005245                 1.000000   \n",
      "comment_score_per_day         0.034106                 0.074263   \n",
      "post_title_word_count         0.015468                -0.075500   \n",
      "contains_event_keyword        0.010462                 0.278396   \n",
      "compound_sentiment           -0.593167                 0.121685   \n",
      "contains_team_name            0.020595                 0.694345   \n",
      "word_count                    0.032371                 0.256465   \n",
      "pos_sentiment                -0.226194                 0.045279   \n",
      "post_score                    0.034005                -0.001334   \n",
      "comment_hour                 -0.013360                 0.042480   \n",
      "post_num_comments             0.022945                 0.264100   \n",
      "comment_score                 0.020217                 0.068262   \n",
      "is_self                       0.056380                 0.344971   \n",
      "upvote_ratio                 -0.046228                -0.037089   \n",
      "days_from_event_start         0.044210                 0.250730   \n",
      "post_title_length             0.013516                -0.060608   \n",
      "author_karma                       NaN                      NaN   \n",
      "contains_hero_keyword         0.022219                -0.022437   \n",
      "\n",
      "                         comment_score_per_day  post_title_word_count  \\\n",
      "contains_question                     0.002035               0.027547   \n",
      "char_count                           -0.007181               0.047088   \n",
      "neu_sentiment                        -0.025686              -0.044322   \n",
      "neg_sentiment                         0.034106               0.015468   \n",
      "contains_player_keyword               0.074263              -0.075500   \n",
      "comment_score_per_day                 1.000000              -0.010319   \n",
      "post_title_word_count                -0.010319               1.000000   \n",
      "contains_event_keyword               -0.004676               0.022462   \n",
      "compound_sentiment                   -0.006002               0.027213   \n",
      "contains_team_name                    0.017179              -0.093582   \n",
      "word_count                           -0.003712               0.048055   \n",
      "pos_sentiment                         0.002802               0.041844   \n",
      "post_score                            0.517011              -0.032122   \n",
      "comment_hour                         -0.047361              -0.027915   \n",
      "post_num_comments                     0.079714              -0.180207   \n",
      "comment_score                         0.689745              -0.034052   \n",
      "is_self                               0.079315              -0.234860   \n",
      "upvote_ratio                         -0.117181               0.143816   \n",
      "days_from_event_start                 0.082603               0.110417   \n",
      "post_title_length                     0.018516               0.974659   \n",
      "author_karma                               NaN                    NaN   \n",
      "contains_hero_keyword                 0.013543               0.042527   \n",
      "\n",
      "                         contains_event_keyword  compound_sentiment  \\\n",
      "contains_question                      0.027547            0.015967   \n",
      "char_count                             0.340823            0.248477   \n",
      "neu_sentiment                         -0.100298           -0.192986   \n",
      "neg_sentiment                          0.010462           -0.593167   \n",
      "contains_player_keyword                0.278396            0.121685   \n",
      "comment_score_per_day                 -0.004676           -0.006002   \n",
      "post_title_word_count                  0.022462            0.027213   \n",
      "contains_event_keyword                 1.000000            0.196987   \n",
      "compound_sentiment                     0.196987            1.000000   \n",
      "contains_team_name                     0.285088            0.115571   \n",
      "word_count                             0.368374            0.263003   \n",
      "pos_sentiment                          0.111421            0.676827   \n",
      "post_score                            -0.053324            0.001286   \n",
      "comment_hour                           0.039356            0.005242   \n",
      "post_num_comments                      0.118015            0.028877   \n",
      "comment_score                         -0.013274           -0.008896   \n",
      "is_self                                0.251394            0.060081   \n",
      "upvote_ratio                          -0.094163            0.012213   \n",
      "days_from_event_start                  0.175039            0.097684   \n",
      "post_title_length                      0.035702            0.029688   \n",
      "author_karma                                NaN                 NaN   \n",
      "contains_hero_keyword                 -0.004936            0.013207   \n",
      "\n",
      "                         contains_team_name  ...  post_score  comment_hour  \\\n",
      "contains_question                  0.022379  ...   -0.012816      0.014004   \n",
      "char_count                         0.222747  ...    0.031795     -0.019148   \n",
      "neu_sentiment                     -0.067693  ...   -0.023591      0.015016   \n",
      "neg_sentiment                      0.020595  ...    0.034005     -0.013360   \n",
      "contains_player_keyword            0.694345  ...   -0.001334      0.042480   \n",
      "comment_score_per_day              0.017179  ...    0.517011     -0.047361   \n",
      "post_title_word_count             -0.093582  ...   -0.032122     -0.027915   \n",
      "contains_event_keyword             0.285088  ...   -0.053324      0.039356   \n",
      "compound_sentiment                 0.115571  ...    0.001286      0.005242   \n",
      "contains_team_name                 1.000000  ...   -0.086601      0.029484   \n",
      "word_count                         0.244769  ...    0.031820     -0.031888   \n",
      "pos_sentiment                      0.064261  ...   -0.000204     -0.002312   \n",
      "post_score                        -0.086601  ...    1.000000     -0.134413   \n",
      "comment_hour                       0.029484  ...   -0.134413      1.000000   \n",
      "post_num_comments                  0.137218  ...    0.140202      0.068764   \n",
      "comment_score                      0.010103  ...    0.550826     -0.056473   \n",
      "is_self                            0.299403  ...   -0.048673      0.120736   \n",
      "upvote_ratio                      -0.052915  ...   -0.120237     -0.082555   \n",
      "days_from_event_start              0.150938  ...    0.117950      0.048608   \n",
      "post_title_length                 -0.086556  ...    0.018061     -0.004065   \n",
      "author_karma                            NaN  ...         NaN           NaN   \n",
      "contains_hero_keyword              0.008316  ...    0.024112     -0.047386   \n",
      "\n",
      "                         post_num_comments  comment_score   is_self  \\\n",
      "contains_question                 0.003549      -0.013400  0.025078   \n",
      "char_count                        0.017785      -0.005910  0.094401   \n",
      "neu_sentiment                    -0.044544       0.012120 -0.073533   \n",
      "neg_sentiment                     0.022945       0.020217  0.056380   \n",
      "contains_player_keyword           0.264100       0.068262  0.344971   \n",
      "comment_score_per_day             0.079714       0.689745  0.079315   \n",
      "post_title_word_count            -0.180207      -0.034052 -0.234860   \n",
      "contains_event_keyword            0.118015      -0.013274  0.251394   \n",
      "compound_sentiment                0.028877      -0.008896  0.060081   \n",
      "contains_team_name                0.137218       0.010103  0.299403   \n",
      "word_count                        0.008564       0.001622  0.104547   \n",
      "pos_sentiment                     0.035752      -0.029864  0.047207   \n",
      "post_score                        0.140202       0.550826 -0.048673   \n",
      "comment_hour                      0.068764      -0.056473  0.120736   \n",
      "post_num_comments                 1.000000       0.137699  0.406249   \n",
      "comment_score                     0.137699       1.000000  0.046560   \n",
      "is_self                           0.406249       0.046560  1.000000   \n",
      "upvote_ratio                      0.097760      -0.098071 -0.185531   \n",
      "days_from_event_start             0.308639       0.079201  0.195890   \n",
      "post_title_length                -0.164000      -0.003230 -0.253297   \n",
      "author_karma                           NaN            NaN       NaN   \n",
      "contains_hero_keyword            -0.050047      -0.009219 -0.082758   \n",
      "\n",
      "                         upvote_ratio  days_from_event_start  \\\n",
      "contains_question           -0.002730               0.013536   \n",
      "char_count                  -0.030914               0.118237   \n",
      "neu_sentiment                0.026765              -0.106164   \n",
      "neg_sentiment               -0.046228               0.044210   \n",
      "contains_player_keyword     -0.037089               0.250730   \n",
      "comment_score_per_day       -0.117181               0.082603   \n",
      "post_title_word_count        0.143816               0.110417   \n",
      "contains_event_keyword      -0.094163               0.175039   \n",
      "compound_sentiment           0.012213               0.097684   \n",
      "contains_team_name          -0.052915               0.150938   \n",
      "word_count                  -0.036746               0.113274   \n",
      "pos_sentiment                0.011797               0.084213   \n",
      "post_score                  -0.120237               0.117950   \n",
      "comment_hour                -0.082555               0.048608   \n",
      "post_num_comments            0.097760               0.308639   \n",
      "comment_score               -0.098071               0.079201   \n",
      "is_self                     -0.185531               0.195890   \n",
      "upvote_ratio                 1.000000              -0.210131   \n",
      "days_from_event_start       -0.210131               1.000000   \n",
      "post_title_length            0.101091               0.156094   \n",
      "author_karma                      NaN                    NaN   \n",
      "contains_hero_keyword        0.107028              -0.034354   \n",
      "\n",
      "                         post_title_length  author_karma  \\\n",
      "contains_question                 0.027370           NaN   \n",
      "char_count                        0.046571           NaN   \n",
      "neu_sentiment                    -0.046688           NaN   \n",
      "neg_sentiment                     0.013516           NaN   \n",
      "contains_player_keyword          -0.060608           NaN   \n",
      "comment_score_per_day             0.018516           NaN   \n",
      "post_title_word_count             0.974659           NaN   \n",
      "contains_event_keyword            0.035702           NaN   \n",
      "compound_sentiment                0.029688           NaN   \n",
      "contains_team_name               -0.086556           NaN   \n",
      "word_count                        0.049805           NaN   \n",
      "pos_sentiment                     0.045629           NaN   \n",
      "post_score                        0.018061           NaN   \n",
      "comment_hour                     -0.004065           NaN   \n",
      "post_num_comments                -0.164000           NaN   \n",
      "comment_score                    -0.003230           NaN   \n",
      "is_self                          -0.253297           NaN   \n",
      "upvote_ratio                      0.101091           NaN   \n",
      "days_from_event_start             0.156094           NaN   \n",
      "post_title_length                 1.000000           NaN   \n",
      "author_karma                           NaN           NaN   \n",
      "contains_hero_keyword             0.017314           NaN   \n",
      "\n",
      "                         contains_hero_keyword  \n",
      "contains_question                    -0.002172  \n",
      "char_count                            0.253023  \n",
      "neu_sentiment                         0.035900  \n",
      "neg_sentiment                         0.022219  \n",
      "contains_player_keyword              -0.022437  \n",
      "comment_score_per_day                 0.013543  \n",
      "post_title_word_count                 0.042527  \n",
      "contains_event_keyword               -0.004936  \n",
      "compound_sentiment                    0.013207  \n",
      "contains_team_name                    0.008316  \n",
      "word_count                            0.233244  \n",
      "pos_sentiment                        -0.052065  \n",
      "post_score                            0.024112  \n",
      "comment_hour                         -0.047386  \n",
      "post_num_comments                    -0.050047  \n",
      "comment_score                        -0.009219  \n",
      "is_self                              -0.082758  \n",
      "upvote_ratio                          0.107028  \n",
      "days_from_event_start                -0.034354  \n",
      "post_title_length                     0.017314  \n",
      "author_karma                               NaN  \n",
      "contains_hero_keyword                 1.000000  \n",
      "\n",
      "[22 rows x 22 columns]\n",
      "2025-07-31 16:16:01,999 - INFO - Plot saved to: c:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\reports\\figures\\pearson_correlation_matrix.png\n",
      "2025-07-31 16:16:02,000 - INFO - \n",
      "--- All EDA Plots Generated ---\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Reddit Scraper for DotA2 Subreddit\n",
    "#\n",
    "# **Guideline:** Online Engagement with Organizational News on Reddit: An Analysis Using the Example of E-Sports Player Transfers and the Online Presence of Organizations in Rankings and Tournaments.\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# #### 1. Project Setup and Data Preparation\n",
    "#\n",
    "# This section handles the initial setup, including importing necessary libraries, and orchestrating the data collection, cleaning, and feature engineering process.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 1.1 Imports and Environment Setup\n",
    "#\n",
    "# Importing all required libraries and configuring the Python environment for modularized functions.\n",
    "\n",
    "# %% [python]\n",
    "import pandas as pd\n",
    "import sys # Required for sys.path manipulation\n",
    "import os  # Required for os.path operations\n",
    "import praw # Required for Reddit API interaction\n",
    "import logging # Required for logging configuration\n",
    "\n",
    "# --- START: Project Root Setup for Module Imports ---\n",
    "# Dynamically determine the project root relative to the notebook's location.\n",
    "# This assumes the notebook is located at:\n",
    "# project_root/BA/notebooks/1_data_extraction/your_notebook.ipynb\n",
    "# So, we need to go up three directories from the current working directory.\n",
    "current_notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the project_root to sys.path if it's not already there.\n",
    "# This must happen *before* importing any modules from the project root (like 'config').\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "# --- END: Project Root Setup for Module Imports ---\n",
    "\n",
    "# Import the centralized configuration\n",
    "import config\n",
    "\n",
    "# --- START: Centralized Logging Configuration for Notebook ---\n",
    "# Ensure the log directory exists before configuring logging\n",
    "os.makedirs(os.path.dirname(config.LOG_FILE), exist_ok=True)\n",
    "\n",
    "# Configure logging once at the application's entry point (the notebook)\n",
    "logging.basicConfig(\n",
    "    level=config.LOG_LEVEL,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOG_FILE), # Log to file\n",
    "        logging.StreamHandler(sys.stdout)     # Log to console/stdout\n",
    "    ]\n",
    ")\n",
    "# --- END: Centralized Logging Configuration for Notebook ---\n",
    "\n",
    "\n",
    "# Import custom modules from src/\n",
    "from BA.src.data.database_utils import load_data_from_sqlite\n",
    "from BA.src.visualization.plots import generate_all_eda_plots, plot_pearson_correlation # Orchestrates all EDA plots, and new correlation plot\n",
    "from BA.src.data.prepare_data import prepare_data # Orchestrates data collection and preprocessing\n",
    "from BA.src.analysis.statistical_tests import perform_independent_t_test, perform_anova_test, perform_chi_squared_test # New statistical tests\n",
    "\n",
    "# For plotting (general setup)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") # Set plot style\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 1.2 Data Collection, Cleaning, and Feature Engineering\n",
    "#\n",
    "# This section orchestrates the entire data preparation pipeline by calling the modularized `prepare_data` function.\n",
    "# This function handles conditional data loading, raw data collection, cleaning, preprocessing, and feature engineering and saves the processed data to the SQLite database. All parameters are now managed via `config.py`.\n",
    "# The Reddit API initialization is now handled internally within `prepare_data.py`.\n",
    "\n",
    "# **Note on Data Collection:**\n",
    "#\n",
    "# The `prepare_data()` function is designed to collect and process data from the Reddit API only if it's not already present in the SQLite database (`reddit_dota2_analysis.db`).This saves time and respects API limits.\n",
    "#\n",
    "# ** IMPORTANT: To apply changes to data processing (like new feature engineering), you MUST delete the existing database file first! **\n",
    "#\n",
    "# For **Windows**:\n",
    "# ```bash\n",
    "# del \"C:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\data\\processed\\reddit_dota2_analysis.db\"\n",
    "# ```\n",
    "#\n",
    "# After deleting the database, `prepare_data()` will automatically collect the data again on its next call.\n",
    "#\n",
    "# If you wish to manually trigger just the full data extraction, preparation, cleaning and feature engineering process, please run the \"prepare_data.py\" script directly in your terminal:\n",
    "#\n",
    "# ```bash\n",
    "# python BA/src/data/prepare_data.py\n",
    "# ```\n",
    "\n",
    "# %% [python]\n",
    "logging.info(\"Starting data preparation and feature engineering pipeline...\")\n",
    "\n",
    "# Call the prepare_data function to execute the full pipeline.\n",
    "# It will return the combined and cleaned DataFrame.\n",
    "# No arguments are passed here, as prepare_data now reads them from config.py\n",
    "df_combined_cleaned = prepare_data()\n",
    "\n",
    "logging.info(\"Data preparation and feature engineering pipeline complete.\")\n",
    "\n",
    "# Confirmation of successful data preparation\n",
    "if not df_combined_cleaned.empty:\n",
    "    logging.info(f\"Combined DataFrame successfully created with shape: {df_combined_cleaned.shape}\")\n",
    "    logging.info(\"Combined Cleaned DataFrame Info:\")\n",
    "    df_combined_cleaned.info() # This prints to stdout, not necessarily to logger\n",
    "    print(\"\\nDescriptive statistics of the combined DataFrame:\")\n",
    "    print(df_combined_cleaned.describe(include='all')) # Display descriptive statistics\n",
    "else:\n",
    "    logging.warning(\"Combined DataFrame is empty after data preparation.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 2. Data Loading and Initial Analysis\n",
    "#\n",
    "# This section focuses on loading the processed data from the database and performing initial analytical steps.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.1 Data Loading from SQLite Database\n",
    "#\n",
    "# Loading the cleaned and feature-engineered data from the previous step by renaming it to df_comments, instead of loading a separate database file with the same data.\n",
    "\n",
    "# %% [python]\n",
    "logging.info(\"\\n--- Loading data from SQLite database ---\")\n",
    "# The df_combined_cleaned DataFrame from the previous step is already loaded and processed.\n",
    "df_comments = df_combined_cleaned\n",
    "\n",
    "if not df_comments.empty:\n",
    "    logging.info(f\"Comments DataFrame loaded with shape: {df_comments.shape}\")\n",
    "    logging.info(\"\\nComments DataFrame Info:\")\n",
    "    df_comments.info() # This prints to stdout, not necessarily to logger\n",
    "    print(\"\\nDescriptive statistics of the loaded DataFrame:\")\n",
    "    print(df_comments.describe(include='all')) # Display descriptive statistics\n",
    "else:\n",
    "    logging.warning(\"No data loaded. Please check previous steps.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.2 Pearson Correlations\n",
    "#\n",
    "# Calculating Pearson correlations between selected numerical features to identify potential relationships.\n",
    "# This analysis is now encapsulated in a dedicated function within `BA/src/visualization/plots.py`.\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Pearson Correlation Matrix\n",
    "#\n",
    "# The Pearson Correlation Matrix provides insights into the linear relationships between pairs of numerical variables. The correlation coefficient ranges from -1 (perfect negative linear correlation) to +1 (perfect positive linear correlation), with 0 indicating no linear correlation.\n",
    "#\n",
    "# **Key Observations:**\n",
    "#\n",
    "# *   **Highly Correlated Features (r > 0.9 or r < -0.9):**\n",
    "#     *   `post_title_word_count` and `post_title_length` (r = 0.97): As expected, longer titles tend to have more words.\n",
    "#     *   `word_count` and `char_count` (r = 0.96): Similarly, comments with more characters naturally have more words.\n",
    "#     *   These high correlations suggest that one of these features might be redundant for certain analyses, though tree-based models like XGBoost are generally robust to multicollinearity.\n",
    "#\n",
    "# *   **Strong Positive Correlations (0.7 < r <= 0.9):**\n",
    "#     *   `contains_team_name` and `contains_player_keyword` (r = 0.69): This indicates that posts mentioning team names often also mention player keywords, suggesting discussions around team rosters or player performances.\n",
    "#     *   `compound_sentiment` and `pos_sentiment` (r = 0.67): A higher positive sentiment score strongly contributes to the overall compound sentiment.\n",
    "#\n",
    "# *   **Moderate Positive Correlations (0.3 < r <= 0.7):**\n",
    "#     *   `comment_score` and `post_score` (r = 0.55): Posts with higher scores tend to attract comments that also receive higher scores. This is a significant relationship for understanding engagement.\n",
    "#     *   `is_self` and `post_num_comments` (r = 0.41): Text-based posts (`is_self`) tend to generate more comments, which is an interesting finding regarding content type and interaction.\n",
    "#     *   `contains_event_keyword` and `word_count` (r = 0.37): Comments containing event-related keywords are moderately correlated with longer comments.\n",
    "#\n",
    "# *   **Strong Negative Correlations (-0.7 <= r < -0.3):**\n",
    "#     *   `pos_sentiment` and `neu_sentiment` (r = -0.72): This is a logical inverse relationship; as positive sentiment increases, neutral sentiment tends to decrease, given that the sum of sentiment scores often approaches 1.\n",
    "#     *   `compound_sentiment` and `neg_sentiment` (r = -0.59): Higher negative sentiment naturally leads to a lower overall compound sentiment.\n",
    "#\n",
    "# *   **Weak or Negligible Correlations (r close to 0):**\n",
    "#     *   Many features show very low correlations with `contains_question` and `comment_hour`. This suggests that there isn't a strong linear relationship between these variables and others in the dataset. It's important to remember that a low linear correlation does not rule out non-linear relationships.\n",
    "#\n",
    "# *   **Notable Observations:**\n",
    "#     *   `author_karma` shows `NaN` for all correlations, indicating missing data for this feature. If author karma is deemed important for the research questions, this data gap needs to be addressed.\n",
    "#     *   `upvote_ratio` shows a very weak negative correlation with `comment_score` (r = -0.097). This might be counter-intuitive, as one might expect highly upvoted posts to have highly scored comments. This could suggest that controversial posts (which might have a lower upvote ratio due to mixed reactions) can still generate significant discussion and highly scored comments.\n",
    "#\n",
    "# **Implications:**\n",
    "# The correlation matrix helps identify potential relationships between features, guiding further analysis and feature selection for predictive modeling. Strong correlations can indicate redundant features or important underlying dynamics. Weak correlations suggest that variables might be independent or have non-linear relationships. It is crucial to remember that correlation does not imply causation.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 2.3 Statistical Tests\n",
    "#\n",
    "# Performing statistical tests to quantitatively assess differences and associations between variables.\n",
    "\n",
    "# %% [python]\n",
    "if not df_comments.empty:\n",
    "    logging.info(\"\\n--- Performing Statistical Tests ---\")\n",
    "\n",
    "    # --- Tests for Research Question: How do different factors influence user interactions? ---\n",
    "\n",
    "    # Independent Samples t-test for Compound Sentiment between two events (e.g., TI8 vs. TI11)\n",
    "    # This test investigates if there's a statistically significant difference in the mean compound sentiment\n",
    "    # between comments related to different International (TI) events.\n",
    "    print(\"\\n--- Independent Samples t-test for Compound Sentiment ---\")\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI8', 'TI11')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Independent Samples t-test (TI8 vs. TI11 for Compound Sentiment)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation basierend auf den geloggten Ergebnissen ein. Erläutern Sie den p-Wert, den t-Wert und die Schlussfolgerung bezüglich der statistischen Signifikanz. Beispiel: \"Der t-Test zwischen TI8 und TI11 für das Compound Sentiment zeigte einen p-Wert von X.XX. Dies deutet auf [einen statistisch signifikanten Unterschied / keinen statistisch signifikanten Unterschied] im Sentiment zwischen diesen beiden Ereignissen hin.\"]\n",
    "\n",
    "# %% [python]\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI8', 'OG_RM24')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Independent Samples t-test (TI8 vs. OG_RM24 for Compound Sentiment)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein.]\n",
    "\n",
    "# %% [python]\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'TI11', 'OG_RM24')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Independent Samples t-test (TI11 vs. OG_RM24 for Compound Sentiment)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein.]\n",
    "\n",
    "# %% [python]\n",
    "    perform_independent_t_test(df_comments, 'event_name', 'compound_sentiment', 'OG_RM24', 'Topson_RM24')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Independent Samples t-test (OG_RM24 vs. Topson_RM24 for Compound Sentiment)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein.]\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### One-Way ANOVA Tests\n",
    "\n",
    "# %% [python]\n",
    "    # One-Way ANOVA for Comment Score across different time periods\n",
    "    # This ANOVA test examines if there are statistically significant differences in the mean comment score\n",
    "    # across the defined time periods relative to an event (Before, During, After, Outside Window).\n",
    "    print(\"\\n--- One-Way ANOVA for Comment Score across Time Periods ---\")\n",
    "    perform_anova_test(df_comments, 'time_period', 'comment_score')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of One-Way ANOVA (Comment Score across Time Periods)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation basierend auf den geloggten Ergebnissen ein. Erläutern Sie den F-Wert, den p-Wert und die Schlussfolgerung bezüglich der statistischen Signifikanz. Beispiel: \"Die ANOVA für den Kommentarpunktwert über die Zeiträume hinweg ergab einen p-Wert von X.XX. Dies deutet auf [einen statistisch signifikanten Unterschied / keinen statistisch signifikanten Unterschied] in den Kommentarpunktwerten zwischen mindestens zwei der Zeiträume hin.\"]\n",
    "\n",
    "# %% [python]\n",
    "    print(\"\\n--- One-Way ANOVA for Compound Sentiment across Time Periods ---\")\n",
    "    perform_anova_test(df_comments, 'time_period', 'compound_sentiment')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of One-Way ANOVA (Compound Sentiment across Time Periods)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein.]\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# ##### Tests for Research Question: How do engagement rates differ between posts on player transfers, tournament results, and ranking placements?\n",
    "#\n",
    "# To address this research question, we perform ANOVA tests on key engagement metrics, grouped by the newly created `post_type` feature.\n",
    "\n",
    "# %% [python]\n",
    "    print(\"\\n--- One-Way ANOVA for Comment Score across Post Types ---\")\n",
    "    perform_anova_test(df_comments, 'post_type', 'comment_score')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of One-Way ANOVA (Comment Score across Post Types)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation basierend auf den geloggten Ergebnissen ein. Erläutern Sie den F-Wert, den p-Wert und die Schlussfolgerung bezüglich der statistischen Signifikanz. Diskutieren Sie, ob es signifikante Unterschiede im durchschnittlichen Kommentarpunktwert zwischen den verschiedenen Beitragstypen (Spielerwechsel, Turnierergebnisse, Ranglistenplatzierungen) gibt.]\n",
    "\n",
    "# %% [python]\n",
    "    print(\"\\n--- One-Way ANOVA for Compound Sentiment across Post Types ---\")\n",
    "    perform_anova_test(df_comments, 'post_type', 'compound_sentiment')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of One-Way ANOVA (Compound Sentiment across Post Types)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein. Diskutieren Sie, ob es signifikante Unterschiede im durchschnittlichen Compound Sentiment zwischen den verschiedenen Beitragstypen gibt.]\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### Chi-squared Tests\n",
    "\n",
    "# %% [python]\n",
    "    # Chi-squared test of independence between 'time_period' and 'contains_question'\n",
    "    # This test determines if there is a statistically significant association between the time period relative to an event and whether a comment contains a question.\n",
    "    print(\"\\n--- Chi-squared test for Time Period and Contains Question ---\")\n",
    "    perform_chi_squared_test(df_comments, 'time_period', 'contains_question')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Chi-squared test (Time Period and Contains Question)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation basierend auf den geloggten Ergebnissen ein. Erläutern Sie den Chi-Quadrat-Wert, den p-Wert und die Schlussfolgerung bezüglich der statistischen Assoziation. Beispiel: \"Der Chi-Quadrat-Test für den Zeitraum und 'contains_question' ergab einen p-Wert von X.XX. Dies deutet auf [eine statistisch signifikante Assoziation / keine statistisch signifikante Assoziation] zwischen diesen beiden Variablen hin.\"]\n",
    "\n",
    "# %% [python]\n",
    "    print(\"\\n--- Chi-squared test for Event Name and Contains Team Name ---\")\n",
    "    perform_chi_squared_test(df_comments, 'event_name', 'contains_team_name')\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Interpretation of Chi-squared test (Event Name and Contains Team Name)\n",
    "#\n",
    "# [Fügen Sie hier Ihre Interpretation ein.]\n",
    "\n",
    "# %% [python]\n",
    "    logging.info(\"\\n--- Statistical Tests Complete ---\")\n",
    "else:\n",
    "    logging.info(\"Comments DataFrame is empty, skipping statistical tests.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 3. Exploratory Data Analysis (EDA) and Visualization\n",
    "#\n",
    "# This section focuses on visually exploring the cleaned and feature-engineered data to identify patterns, trends, and differences between the four variables. All plots are generated via a single orchestrating function.\n",
    "\n",
    "# %% [markdown]\n",
    "# ##### 3.1 Interpretation of Distribution Plots\n",
    "#\n",
    "# When analyzing the distribution plots (histograms with KDE, and the new Box/Violin plots), focus on the following aspects for a comprehensive understanding:\n",
    "#\n",
    "# *   **KDE Curve (Kernel Density Estimate):** This smoothed line is crucial for understanding the *shape* of the distribution. Look for:\n",
    "#     *   **Peaks (Modes):** Where are the most frequent values concentrated? Are there multiple peaks?\n",
    "#     *   **Skewness:** Is the distribution symmetrical, or is it skewed to the left (tail on the left) or right (tail on the right)?\n",
    "#     *   **Spread:** How wide is the distribution? Does it cover a large range of values or are they tightly clustered?\n",
    "#     *   **Comparison:** When comparing multiple KDE curves (e.g., for different events), observe if their shapes are similar, if their peaks align, and if their spreads are comparable.\n",
    "#\n",
    "# *   **Box Plots / Violin Plots:** These plots are excellent for comparing distributions *between different groups* (e.g., events).\n",
    "#     *   **Box Plots:** Clearly show the median (middle line), the interquartile range (IQR, the box itself, representing the middle 50% of data), and potential outliers (individual points). They are great for quickly assessing central tendency, spread, and skewness.\n",
    "#     *   **Violin Plots:** Combine the box plot with the KDE, showing the density of the data at different values. They provide a richer view of the distribution's shape than box plots, especially useful for identifying multimodal distributions.\n",
    "#\n",
    "# *   **Quantitative Metrics (Mean, Median, Standard Deviation):** These numerical summaries, provided next to the plots, are essential for precise, quantitative comparisons.\n",
    "#     *   **Mean & Median:** Indicate the *central tendency* of the data. Compare these values across groups to see if the \"average\" or \"typical* value differs.\n",
    "#     *   **Standard Deviation:** Measures the *spread* or *variability* of the data. A larger standard deviation means data points are more spread out from the mean.\n",
    "#\n",
    "# **In summary:** While histograms give a raw count, the **KDE curve** helps you understand the *overall shape* of the distribution. The **Box/Violin plots** are superior for *comparing distributions across groups*. The **numerical summary statistics** provide the *precise quantitative details* to support your visual observations and are critical for your analysis.\n",
    "\n",
    "# %% [python]\n",
    "# Generate all EDA plots using the modularized function\n",
    "if not df_comments.empty:\n",
    "    generate_all_eda_plots(df_comments)\n",
    "else:\n",
    "    logging.info(\"Comments DataFrame is empty, skipping EDA plots.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 4. Machine Learning Preparation\n",
    "#\n",
    "# This section outlines the preparation steps for machine learning, with core functionalities modularized into external scripts, which is found within the `BA/src/models/train_model.py` script.\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 5. Model Preparation and Data Splitting\n",
    "#\n",
    "# The process of splitting the data into training and testing sets, as well as the final assembly of the feature matrix, is handled within the `BA/src/models/train_model.py` script.\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "#\n",
    "# #### 6. Model Training and Evaluation\n",
    "#\n",
    "# All aspects of model training, hyperparameter tuning, cross-validation, and model evaluation are fully modularized and orchestrated by the `BA/src/models/train_model.py` script.\n",
    "# The machine learning pipeline is designed to be executed as a standalone script, ensuring a clean and reproducible workflow.\n",
    "# To execute the full machine learning pipeline, please run the `train_model.py` script directly from your terminal:\n",
    "#\n",
    "# ```bash\n",
    "# python \"C:\\Users\\Destiny\\OneDrive - HWR Berlin\\Desktop\\Schule\\HWR\\1 Abschlussarbeit\\Code\\BA\\src\\models\\train_model.py\"\n",
    "# ```\n",
    "#\n",
    "# This modular approach enhances reproducibility, maintainability, and allows for easier integration into automated workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
